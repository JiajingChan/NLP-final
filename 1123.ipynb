{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "678a7b49-66d2-47ee-8c00-d29cacb92784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. build our own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2116af7-acbd-4fdf-ab5e-80ef11677cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'POS', 'BIO', 'label', 'id'],\n",
       "        num_rows: 2174\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'POS', 'BIO', 'label', 'id'],\n",
       "        num_rows: 83\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'POS', 'BIO', 'label', 'id'],\n",
       "        num_rows: 150\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "# map POS to POS_id\n",
    "cnt = 0\n",
    "POS_id = {}\n",
    "POS_ls = ['NN', 'IN', 'NNP', 'DT', 'NNS', 'JJ', 'COMMA', 'CD', '.', 'VBD', 'RB','VB', 'CC', 'VBN', 'VBZ', \n",
    "          'VBG', 'TO', 'PRP', 'VBP', 'POS', 'PRP$','MD', '$', '``', \"''\", 'WDT', ':', 'JJR', 'RP', 'RBR', \n",
    "          'WP', 'NNPS','JJS', ')', '(', 'EX', 'RBS', 'WRB', '-', 'UH', 'WP$', 'PDT', '/', '#', 'LS', 'SYM', 'FW', 'AUX']\n",
    "for pos in POS_ls:\n",
    "    POS_id[pos] = cnt\n",
    "    cnt += 1\n",
    "\n",
    "# map BIO to BIO_id\n",
    "cnt = 0\n",
    "BIO_id = {}\n",
    "BIO_ls = ['O', 'B-NP', 'I-NP', 'B-PP', 'B-ADVP', 'B-ADJP', 'B-SBAR', 'B-CONJP',\n",
    "       'I-ADJP', 'I-PP', 'I-ADVP', 'I-CONJP', 'B-INTJ', 'I-SBAR', 'B-LST',\n",
    "       'B-VP', 'B-PRT', 'I-INTJ', 'I-VP']\n",
    "for bio in BIO_ls:\n",
    "    BIO_id[bio] = cnt\n",
    "    cnt += 1\n",
    "\n",
    "# map label to BIO_id\n",
    "Label_id = {\"ARG0\":0,\"ARG1\":1,\"ARG2\":2,\"PRED\":3,\"SUPPORT\":4}\n",
    "def mapLabel(label):\n",
    "    return Label_id[label] if label in Label_id else 5\n",
    "\n",
    "# build datasets\n",
    "def condense_df(file):\n",
    "    df = pd.DataFrame()\n",
    "    with open(file, 'r') as file:\n",
    "        ls = [i.split('\\t') for i in file.read().split('\\n')]\n",
    "        df = pd.DataFrame(ls)\n",
    "\n",
    "    df['id'] = df.index\n",
    "    df[0].replace('', None, inplace=True)\n",
    "    df.dropna(axis=0, subset = [0], inplace = True)\n",
    "    df['BIO'] = df[2].map(BIO_id)\n",
    "    df['POS'] = df[1].map(POS_id)\n",
    "    df['label'] = df[5].map(mapLabel)\n",
    "    df['id'] = df[4].map(int)\n",
    "    df.drop(columns = [1, 2, 3, 4, 5, 6], inplace = True)\n",
    "    condense = df.groupby('id').apply(lambda x: [list(x[0]),list(x['POS']), list(x['BIO']), list(x['label'])]).apply(pd.Series)\n",
    "    condense.columns =['tokens','POS','BIO','label']\n",
    "    return condense\n",
    "\n",
    "\n",
    "train = Dataset.from_pandas(condense_df(\"Partitive-Files/%_nombank.clean.train\"))\n",
    "eval_ = Dataset.from_pandas(condense_df(\"Partitive-Files/%_nombank.clean.dev\"))\n",
    "test = Dataset.from_pandas(condense_df(\"Partitive-Files/%_nombank.clean.test\"))\n",
    "datasets = DatasetDict({\"train\": train, \"validation\":eval_, \"test\":test})\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad2a9519-741f-4eb3-a074-2af5e116fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9322db92-e1ac-4c82-9f7c-86d7f9b75943",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc35fd7fdcf460a960d2653fac037d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b8503a214c456595c5dbc3d6cc1f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eab0fc4a5c44edca0df59e9166c130a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if not word_id:\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            if word_id != current_word:# Start of a new word!\n",
    "                current_word = word_id       \n",
    "            label = labels[word_id]\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    all_labels = examples[\"label\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns = datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa27677f-cf86-4752-b389-505d32e45f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. train while evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d03d8e7-778d-4d0f-b6d2-d5c940a71b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(100))\n",
    "small_eval = tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(30))\n",
    "small_test = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49e9dc1c-9c4b-4a3f-a334-44ce8203c1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\"ARG0\", \"ARG1\", \"ARG2\", \"PRED\", \"SUPPORT\", \"None\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb96307d-f409-4bf5-9cb2-63557118360b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/jiajingchen/miniforge3/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2174\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 816\n",
      "  Number of trainable parameters = 107724294\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='816' max='816' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [816/816 09:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.069533</td>\n",
       "      <td>0.725177</td>\n",
       "      <td>0.834694</td>\n",
       "      <td>0.776091</td>\n",
       "      <td>0.969301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.085500</td>\n",
       "      <td>0.065171</td>\n",
       "      <td>0.776876</td>\n",
       "      <td>0.781633</td>\n",
       "      <td>0.779247</td>\n",
       "      <td>0.971418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.085500</td>\n",
       "      <td>0.065721</td>\n",
       "      <td>0.749071</td>\n",
       "      <td>0.822449</td>\n",
       "      <td>0.784047</td>\n",
       "      <td>0.971066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 83\n",
      "  Batch size = 8\n",
      "/Users/jiajingchen/miniforge3/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: None seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/jiajingchen/miniforge3/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRED seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/jiajingchen/miniforge3/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SUPPORT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/jiajingchen/miniforge3/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ARG1 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "Saving model checkpoint to bert-finetuned/checkpoint-272\n",
      "Configuration saved in bert-finetuned/checkpoint-272/config.json\n",
      "Model weights saved in bert-finetuned/checkpoint-272/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned/checkpoint-272/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned/checkpoint-272/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 83\n",
      "  Batch size = 8\n",
      "/Users/jiajingchen/miniforge3/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: None seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/jiajingchen/miniforge3/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRED seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/jiajingchen/miniforge3/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SUPPORT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/jiajingchen/miniforge3/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ARG1 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "Saving model checkpoint to bert-finetuned/checkpoint-544\n",
      "Configuration saved in bert-finetuned/checkpoint-544/config.json\n",
      "Model weights saved in bert-finetuned/checkpoint-544/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned/checkpoint-544/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned/checkpoint-544/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 83\n",
      "  Batch size = 8\n",
      "/Users/jiajingchen/miniforge3/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: None seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/jiajingchen/miniforge3/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRED seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/jiajingchen/miniforge3/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SUPPORT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/jiajingchen/miniforge3/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ARG1 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "Saving model checkpoint to bert-finetuned/checkpoint-816\n",
      "Configuration saved in bert-finetuned/checkpoint-816/config.json\n",
      "Model weights saved in bert-finetuned/checkpoint-816/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned/checkpoint-816/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned/checkpoint-816/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=816, training_loss=0.06685576251908845, metrics={'train_runtime': 577.1336, 'train_samples_per_second': 11.301, 'train_steps_per_second': 1.414, 'total_flos': 207936455044752.0, 'train_loss': 0.06685576251908845, 'epoch': 3.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }\n",
    "id2label = {str(i): label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir = \"bert-finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = args,\n",
    "    train_dataset = tokenized_datasets[\"train\"],#small_train,#\n",
    "    eval_dataset = tokenized_datasets[\"validation\"],#small_eval,#\n",
    "    data_collator = data_collator,\n",
    "    compute_metrics = compute_metrics,\n",
    "    tokenizer = tokenizer,\n",
    ")\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80ce07ff-fed0-4a6c-9b94-25f54b05357c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file bert-finetuned/checkpoint-816/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-finetuned/checkpoint-816\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"ARG0\",\n",
      "    \"1\": \"ARG1\",\n",
      "    \"2\": \"ARG2\",\n",
      "    \"3\": \"PRED\",\n",
      "    \"4\": \"SUPPORT\",\n",
      "    \"5\": \"None\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"ARG0\": \"0\",\n",
      "    \"ARG1\": \"1\",\n",
      "    \"ARG2\": \"2\",\n",
      "    \"None\": \"5\",\n",
      "    \"PRED\": \"3\",\n",
      "    \"SUPPORT\": \"4\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file bert-finetuned/checkpoint-816/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForTokenClassification.\n",
      "\n",
      "All the weights of BertForTokenClassification were initialized from the model checkpoint at bert-finetuned/checkpoint-816.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 150\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loading the model you previously trained\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-finetuned/checkpoint-816\")\n",
    "BATCH_SIZE = 1\n",
    "# arguments for Trainer\n",
    "test_args = TrainingArguments(\n",
    "    output_dir = \"bert-finetuned-testing\",\n",
    "    do_train = False,\n",
    "    do_predict = True,\n",
    "    per_device_eval_batch_size = BATCH_SIZE,   \n",
    "    dataloader_drop_last = False    \n",
    ")\n",
    "\n",
    "# init trainer\n",
    "trainer = Trainer(\n",
    "          model = model, \n",
    "          args = test_args, \n",
    "          compute_metrics = compute_metrics)\n",
    "\n",
    "test_results = trainer.predict(tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10fef61e-7dbd-4574-873a-e202a00f297f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.05072753131389618,\n",
       " 'test_precision': 0.801123595505618,\n",
       " 'test_recall': 0.8176605504587156,\n",
       " 'test_f1': 0.8093076049943245,\n",
       " 'test_accuracy': 0.9759927797833935,\n",
       " 'test_runtime': 7.1178,\n",
       " 'test_samples_per_second': 21.074,\n",
       " 'test_steps_per_second': 21.074}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcecdc5f-ec97-4974-8170-7817127660f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
