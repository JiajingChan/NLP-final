{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "678a7b49-66d2-47ee-8c00-d29cacb92784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. build our own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff3c14f-c9d7-4faf-a935-99393ce2d33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# map POS to POS_id\n",
    "cnt = 0\n",
    "POS_id = {}\n",
    "POS_ls = ['NN', 'IN', 'NNP', 'DT', 'NNS', 'JJ', 'COMMA', 'CD', '.', 'VBD', 'RB','VB', 'CC', 'VBN', 'VBZ', \n",
    "          'VBG', 'TO', 'PRP', 'VBP', 'POS', 'PRP$','MD', '$', '``', \"''\", 'WDT', ':', 'JJR', 'RP', 'RBR', \n",
    "          'WP', 'NNPS','JJS', ')', '(', 'EX', 'RBS', 'WRB', '-', 'UH', 'WP$', 'PDT', '/', '#', 'LS', 'SYM', 'FW', 'AUX']\n",
    "for pos in POS_ls:\n",
    "    POS_id[pos] = cnt\n",
    "    cnt += 1\n",
    "\n",
    "# map BIO to BIO_id\n",
    "cnt = 0\n",
    "BIO_id = {}\n",
    "BIO_ls = ['O', 'B-NP', 'I-NP', 'B-PP', 'B-ADVP', 'B-ADJP', 'B-SBAR', 'B-CONJP',\n",
    "       'I-ADJP', 'I-PP', 'I-ADVP', 'I-CONJP', 'B-INTJ', 'I-SBAR', 'B-LST',\n",
    "       'B-VP', 'B-PRT', 'I-INTJ', 'I-VP']\n",
    "for bio in BIO_ls:\n",
    "    BIO_id[bio] = cnt\n",
    "    cnt += 1\n",
    "print(len(POS_ls))\n",
    "print(len(BIO_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2116af7-acbd-4fdf-ab5e-80ef11677cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'POS', 'BIO', 'label', 'id'],\n",
       "        num_rows: 2174\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'POS', 'BIO', 'label', 'id'],\n",
       "        num_rows: 83\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'POS', 'BIO', 'label', 'id'],\n",
       "        num_rows: 150\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# map label to BIO_id\n",
    "Label_id = {\"ARG0\":0,\"ARG1\":1,\"ARG2\":2,\"PRED\":3,\"SUPPORT\":4}\n",
    "def mapLabel(label):\n",
    "    return Label_id[label] if label in Label_id else 5\n",
    "\n",
    "# build datasets\n",
    "def condense_df(file):\n",
    "    df = pd.DataFrame()\n",
    "    with open(file, 'r') as file:\n",
    "        ls = [i.split('\\t') for i in file.read().split('\\n')]\n",
    "        df = pd.DataFrame(ls)\n",
    "\n",
    "    df['id'] = df.index\n",
    "    df[0].replace('', np.nan, inplace=True)\n",
    "    df.dropna(axis=0, subset = [0], inplace = True)\n",
    "    df['BIO'] = df[2].map(BIO_id)\n",
    "    df['POS'] = df[1].map(POS_id)\n",
    "    df['label'] = df[5].map(mapLabel)\n",
    "    df['id'] = df[4].map(int)\n",
    "    df.drop(columns = [1, 2, 3, 4, 5, 6], inplace = True)\n",
    "    condense = df.groupby('id').apply(lambda x: [list(x[0]),list(x['POS']), list(x['BIO']), list(x['label'])]).apply(pd.Series)\n",
    "    condense.columns =['tokens','POS','BIO','label']\n",
    "    return condense\n",
    "\n",
    "\n",
    "train = Dataset.from_pandas(condense_df(\"Partitive-Files/%_nombank.clean.train\"))\n",
    "eval_ = Dataset.from_pandas(condense_df(\"Partitive-Files/%_nombank.clean.dev\"))\n",
    "test = Dataset.from_pandas(condense_df(\"Partitive-Files/%_nombank.clean.test\"))\n",
    "\n",
    "\n",
    "datasets = DatasetDict({\"train\": train, \"validation\":eval_, \"test\":test})\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad2a9519-741f-4eb3-a074-2af5e116fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9322db92-e1ac-4c82-9f7c-86d7f9b75943",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3278dfa69284d08b93f78e897eb04f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2d202eb6864fc49df2b388bd66530e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f75e0c4153746ad84a30e4363b13130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def align_labels_with_tokens(labels, POSs, BIOs, word_ids):\n",
    "    new_labels = []\n",
    "    POS_labels = []\n",
    "    BIO_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if not word_id:\n",
    "            new_labels.append(-100)\n",
    "            POS_labels.append(-100)\n",
    "            BIO_labels.append(-100)\n",
    "        else:\n",
    "            if word_id != current_word:# Start of a new word!\n",
    "                current_word = word_id       \n",
    "            new_labels.append(labels[word_id])\n",
    "            POS_labels.append(POSs[word_id])\n",
    "            BIO_labels.append(BIOs[word_id])\n",
    "\n",
    "    return new_labels, POS_labels, BIO_labels\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    all_labels = examples[\"label\"]\n",
    "    all_POS = examples[\"POS\"]\n",
    "    all_BIO = examples[\"BIO\"]\n",
    "    \n",
    "    new_labels = []\n",
    "    POS_labels = []\n",
    "    BIO_labels = []\n",
    "    for i in range(len(all_labels)):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_label, POS_label,BIO_label = align_labels_with_tokens(all_labels[i], all_POS[i], all_BIO[i], word_ids)\n",
    "        new_labels.append(new_label)\n",
    "        POS_labels.append(POS_label)\n",
    "        BIO_labels.append(BIO_label)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    tokenized_inputs[\"POS\"] = POS_labels\n",
    "    tokenized_inputs[\"BIO\"] = BIO_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns = datasets[\"train\"].column_names,\n",
    ")\n",
    "\"\"\"\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if not word_id:\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            if word_id != current_word:# Start of a new word!\n",
    "                current_word = word_id       \n",
    "            label = labels[word_id]\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    all_labels = examples[\"label\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns = datasets[\"train\"].column_names,\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "575a2382-9f6c-41a4-a3b3-66c07e78e1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'POS', 'BIO', 'label', 'id'],\n",
       "        num_rows: 2174\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'POS', 'BIO', 'label', 'id'],\n",
       "        num_rows: 83\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'POS', 'BIO', 'label', 'id'],\n",
       "        num_rows: 150\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "813b00f9-057f-483b-94d2-cd1cf154491c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2174\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 83\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 150\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e660c689-1dd0-43ff-96ae-76322a39dfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for target in ['train','validation','test']:\n",
    "#    for add in ['POS', 'BIO']:\n",
    "#        tokenized_datasets[target] = tokenized_datasets[target].add_column(add, datasets[target][add])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793fbd1e-2a90-4e98-9730-21670d9e7ac8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'POS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5d/l9sqzgw56mv0n0wg3kfxh0840000gn/T/ipykernel_5322/2576962373.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenized_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'POS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'POS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'POS'"
     ]
    }
   ],
   "source": [
    "#tokenized_datasets['train'][100]['POS']\n",
    "#print(len(tokenized_datasets['train'][100]['POS']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5206b0a-1e2d-4e6c-99b5-3e8e840c141a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_datasets['train'][100]['attention_mask']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ce6bbd-eabf-41ef-8993-8f95695c6630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets['train'][100]['input_ids']\n",
    "print(len(tokenized_datasets['train'][100]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bccebac-a1fc-4ff9-8fff-cfd1405f6f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[101, 2021, 2055, 2423, 1003, 1997, 1996, 2529...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, -100, 5, 5, 3, 5, 5, 1, 1, 5, 5, 5, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[101, 1996, 5416, 4636, 2097, 15697, 1999, 215...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, -100, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[101, 1996, 2597, 1997, 1996, 2142, 2163, 4012...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, -100, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[101, 1996, 2034, 8893, 9781, 8079, 2550, 2478...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, -100, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[101, 2720, 1012, 3389, 15154, 2056, 1996, 889...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, -100, -100, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2169</th>\n",
       "      <td>[101, 3653, 2696, 2595, 4082, 5618, 2013, 7026...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, -100, -100, -100, 5, 1, 5, 5, 5, 4, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2170</th>\n",
       "      <td>[101, 12594, 3123, 1022, 1012, 1022, 1003, 200...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, -100, 4, 5, 5, 5, 3, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2171</th>\n",
       "      <td>[101, 1996, 2194, 2056, 1996, 4284, 2443, 1037...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, -100, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2172</th>\n",
       "      <td>[101, 1996, 2194, 2056, 1996, 4284, 2443, 1037...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, -100, 5, 5, 5, 5, 5, 5, 5, 3, 4, 5, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2173</th>\n",
       "      <td>[101, 6599, 1999, 1996, 12108, 3688, 1998, 257...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, -100, 5, 5, 5, 5, 5, 5, 5, 4, 5, 3, 5, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2174 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              input_ids  \\\n",
       "0     [101, 2021, 2055, 2423, 1003, 1997, 1996, 2529...   \n",
       "1     [101, 1996, 5416, 4636, 2097, 15697, 1999, 215...   \n",
       "2     [101, 1996, 2597, 1997, 1996, 2142, 2163, 4012...   \n",
       "3     [101, 1996, 2034, 8893, 9781, 8079, 2550, 2478...   \n",
       "4     [101, 2720, 1012, 3389, 15154, 2056, 1996, 889...   \n",
       "...                                                 ...   \n",
       "2169  [101, 3653, 2696, 2595, 4082, 5618, 2013, 7026...   \n",
       "2170  [101, 12594, 3123, 1022, 1012, 1022, 1003, 200...   \n",
       "2171  [101, 1996, 2194, 2056, 1996, 4284, 2443, 1037...   \n",
       "2172  [101, 1996, 2194, 2056, 1996, 4284, 2443, 1037...   \n",
       "2173  [101, 6599, 1999, 1996, 12108, 3688, 1998, 257...   \n",
       "\n",
       "                                         token_type_ids  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "2169  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2170  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2171  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2172  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2173  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                         attention_mask  \\\n",
       "0     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                 ...   \n",
       "2169  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2170  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2171  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2172  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2173  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                                 labels  \n",
       "0     [-100, -100, 5, 5, 3, 5, 5, 1, 1, 5, 5, 5, 5, ...  \n",
       "1     [-100, -100, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...  \n",
       "2     [-100, -100, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...  \n",
       "3     [-100, -100, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...  \n",
       "4     [-100, -100, -100, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...  \n",
       "...                                                 ...  \n",
       "2169  [-100, -100, -100, -100, 5, 1, 5, 5, 5, 4, 5, ...  \n",
       "2170  [-100, -100, 4, 5, 5, 5, 3, 5, 5, 5, 5, 5, 5, ...  \n",
       "2171  [-100, -100, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...  \n",
       "2172  [-100, -100, 5, 5, 5, 5, 5, 5, 5, 3, 4, 5, 5, ...  \n",
       "2173  [-100, -100, 5, 5, 5, 5, 5, 5, 5, 4, 5, 3, 5, ...  \n",
       "\n",
       "[2174 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.DataFrame(tokenized_datasets[\"train\"])\n",
    "a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c7cb01-3954-4752-8dcd-065b9ed12b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(POS_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97baea1a-7bc7-404c-93b2-b11373090f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_len = list(a['POS'].map(len))\n",
    "BIO_len = list(a['BIO'].map(len))\n",
    "input_ids_len = list(a['input_ids'].map(len))\n",
    "token_type_ids_len = list(a['token_type_ids'].map(len))\n",
    "attention_mask_len = list(a['attention_mask'].map(len))\n",
    "labels_len = list(a['labels'].map(len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ec167a-e39f-4523-86e5-fd9c9f07d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_len == token_type_ids_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fd75bd7-be3f-4eae-aaf2-99fedf9d9810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator#\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4766975-ca3c-4801-8e87-badd3aa8e991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 2174\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dea31658-1a42-4dc6-89d6-dd9eeaaf5ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,batch in enumerate(train_dataloader):\n",
    "    pass\n",
    "    \"\"\"\n",
    "    print(\"input_ids\",batch['input_ids'].size())\n",
    "    print(\"token_type_ids\",batch['token_type_ids'].size())\n",
    "    print(\"attention_mask\",batch['attention_mask'].size())\n",
    "    print(\"labels\",batch['labels'].size())\n",
    "    print(\"POS\",batch['POS'].size())\n",
    "    print(\"BIO\",batch['BIO'].size())\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74815de-13bb-4a67-a5ba-2ad487f939e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_datasets['train'])\n",
    "print(tokenized_datasets['validation'])\n",
    "print(tokenized_datasets['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5bb72b-941a-46de-8b81-304dc93c503b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tokenized_datasets['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dcee1a-9723-488a-b24d-4fe24087e9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mxLen, mxIdx = 0, 0\n",
    "for idx, x in enumerate(tokenized_datasets[\"train\"]['input_ids']):\n",
    "    if mxLen < len(x):\n",
    "        mxLen = len(x)\n",
    "        mxIdx = idx\n",
    "\n",
    "print(mxLen)\n",
    "print(mxIdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d863e5c-370e-4d3c-84f6-b8db635eec8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example = datasets['train'][286]['tokens']\n",
    "tokenized_input = tokenizer(example, is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(\"the original length is: \", len(example))\n",
    "print(\"the original example is: \", example)\n",
    "print(\"the tokenized output is: \", tokens)\n",
    "print(\"the tokenized length is: \", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a280152e-01b5-4224-ad56-ee490d45e73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690e25ca-86b5-41dc-ad07-0c30f6e41a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import missingno as msno\n",
    "msno.matrix(pd.DataFrame(tokenized_datasets[\"train\"]['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa27677f-cf86-4752-b389-505d32e45f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. train while evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d03d8e7-778d-4d0f-b6d2-d5c940a71b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "small_train = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(64))\n",
    "small_eval = tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(16))\n",
    "small_test = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(8))\n",
    "\n",
    "\n",
    "BatchSize = 8\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    small_train, shuffle=True, batch_size=BatchSize, collate_fn=data_collator#tokenized_datasets[\"train\"]\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    small_eval, batch_size=BatchSize, collate_fn=data_collator#tokenized_datasets[\"validation\"]\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    small_test, batch_size=BatchSize, collate_fn=data_collator#tokenized_datasets[\"test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44fb6b2-6988-4c5c-98e0-fcf7b15b240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(small_train['POS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c36029-9724-47ac-b2ef-f45acf6e9a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(small_train['POS'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c02d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\"ARG0\", \"ARG1\", \"ARG2\", \"PRED\", \"SUPPORT\", \"None\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d8796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "import torch.nn as nn\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self,checkpoint,num_labels): \n",
    "        super(CustomModel,self).__init__() \n",
    "        self.num_labels = num_labels \n",
    "\n",
    "        #Load Model with given checkpoint and extract its body\n",
    "        self.model = AutoModel.from_pretrained(checkpoint,config=AutoConfig.from_pretrained(checkpoint, output_attentions=True,output_hidden_states=True))\n",
    "        self.dropout = nn.Dropout(0.1) \n",
    "        self.classifier = nn.Linear(768,num_labels) # load and initialize weights\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, labels):\n",
    "        #TODO: add 2 more parameters as: forward(self, input_ids, token_type_ids, attention_mask, POS, BIO, labels)\n",
    "        #Extract outputs from the body\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         print(\"input_ids:::\"+str(input_ids.shape))\n",
    "#         print(\"labels:::\"+str(labels.shape))\n",
    "        #Add custom layers\n",
    "        sequence_output = self.dropout(outputs[0]) #outputs[0]=last hidden state\n",
    "#         print(\"sequence_output:::\"+str(sequence_output.shape))\n",
    "        #sequence_output = outputs\n",
    "        print(sequence_output.size())\n",
    "        \"\"\"\n",
    "        TODO: concat\n",
    "        After using print(sequence_output.size()) here, we have:\n",
    "        torch.Size([8, 44, 768])\n",
    "        torch.Size([8, 50, 768]) etc.\n",
    "        第一维度是全局batch_size我设置成8\n",
    "        第二维是一句话里面token的个数，是经过分割OOV过后的token数量\n",
    "        第三维是Bert的输出\n",
    "        \n",
    "        假设POS 48维，BIO 10维，那么总的第三维在concat后的总维数是 768+48+10，concat不会改变第一维和第二维的长度\n",
    "        \n",
    "        logits = self.classifier(sequence_output[:,:,:].view(-1, 768+48+10)) # calculate losses\n",
    "        \"\"\"\n",
    "        \n",
    "        logits = self.classifier(sequence_output[:,:,:].view(-1, 768)) # calculate losses\n",
    "        self.logits = logits\n",
    "        #print(\"logits::\"+str(logits.shape))\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states,attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0c4d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW,get_scheduler\n",
    "from datasets import load_metric\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "num_epochs = 3\n",
    "model_cc = CustomModel(checkpoint=model_checkpoint,num_labels=len(label_names))#.cuda()\n",
    "optimizer = AdamW(model_cc.parameters(), lr=2e-5)\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "progress_bar_train = tqdm(range(num_training_steps),miniters=2)\n",
    "progress_bar_eval = tqdm(range(num_epochs * len(eval_dataloader)),miniters=2)\n",
    "f1_best = 0\n",
    "resume_flag = False\n",
    "best_net = None\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    if resume_flag:\n",
    "        model_cc.load_state_dict(torch.load(\"sstcls_best.dat\"))\n",
    "    model_cc.train()\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v for k, v in batch.items()}#.cuda()\n",
    "        outputs = model_cc(batch['input_ids'], token_type_ids=batch['token_type_ids'], attention_mask=batch['attention_mask'],labels=batch['labels'])\n",
    "        #TODO: add 2 more parameters as followed:\n",
    "        #model_cc(batch['input_ids'], token_type_ids=batch['token_type_ids'], attention_mask=batch['attention_mask'],POS = batch['POS'], BIO = batch['BIO'], labels=batch['labels'])\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar_train.update(1)\n",
    "    model_cc.eval()\n",
    "    f1_now = []\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v for k, v in batch.items()}#.cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs = model_cc(batch['input_ids'], token_type_ids=batch['token_type_ids'], attention_mask=batch['attention_mask'],labels=batch['labels'])\n",
    "            #TODO: add 2 more parameters as followed:\n",
    "            #model_cc(batch['input_ids'], token_type_ids=batch['token_type_ids'], attention_mask=batch['attention_mask'],POS = batch['POS'], BIO = batch['BIO'], labels=batch['labels'])\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        true_labels = [[label_names[l] for l in label if l != -100] for label in batch[\"labels\"]]\n",
    "        \n",
    "        reshaped_predictions = torch.reshape(predictions, (BatchSize,-1))\n",
    "        true_predictions = [\n",
    "            [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(reshaped_predictions, batch[\"labels\"])\n",
    "        ]\n",
    "        \n",
    "        all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "        progress_bar_eval.update(1)\n",
    "        f1_now.append(all_metrics[\"overall_f1\"])\n",
    "        \n",
    "    if np.mean(f1_now) > f1_best or not best_net:\n",
    "        torch.save(model_cc.state_dict(), 'sstcls_best.dat')\n",
    "        f1_best = np.mean(f1_now)\n",
    "        print(\"the best f1 is now: \"+ str(np.mean(f1_now)))\n",
    "        best_net = model_cc\n",
    "        \n",
    "\n",
    "    print(metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dea989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, dataloader, gpu):\n",
    "    net.eval()\n",
    "\n",
    "    f1_now = []\n",
    "    precision_now = []\n",
    "    recall_now = []\n",
    "    accuracy_now = []\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v for k, v in batch.items()}#.cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs = model_cc(batch['input_ids'], token_type_ids=batch['token_type_ids'], attention_mask=batch['attention_mask'], labels=batch['labels'])\n",
    "            #TODO: add 2 more parameters as followed:\n",
    "            #model_cc(batch['input_ids'], token_type_ids=batch['token_type_ids'], attention_mask=batch['attention_mask'],POS = batch['POS'], BIO = batch['BIO'], labels=batch['labels'])\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        true_labels = [[label_names[l] for l in label if l != -100] for label in batch[\"labels\"]]\n",
    "        \n",
    "        reshaped_predictions = torch.reshape(predictions, (BatchSize,-1))\n",
    "        true_predictions = [\n",
    "            [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(reshaped_predictions, batch[\"labels\"])\n",
    "        ]\n",
    "        \n",
    "        all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "        progress_bar_eval.update(1)\n",
    "        f1_now.append(all_metrics[\"overall_f1\"])\n",
    "        precision_now.append(all_metrics[\"overall_precision\"])\n",
    "        recall_now.append(all_metrics[\"overall_recall\"])\n",
    "        accuracy_now.append(all_metrics[\"overall_accuracy\"])\n",
    "\n",
    "\n",
    "    return np.mean(f1_now), np.mean(precision_now), np.mean(recall_now), np.mean(accuracy_now),\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca124a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict(best_net, test_dataloader, 0)\n",
    "print(\"test_f1_score:::\"+str(preds[0]))\n",
    "print(\"test_precision_score:::\"+str(preds[1]))\n",
    "print(\"test_recall_score:::\"+str(preds[2]))\n",
    "print(\"test_accuracy_score:::\"+str(preds[3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
