{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "678a7b49-66d2-47ee-8c00-d29cacb92784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. build our own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff3c14f-c9d7-4faf-a935-99393ce2d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# map POS to POS_id\n",
    "cnt = 0\n",
    "POS_id = {}\n",
    "POS_ls = ['NN', 'IN', 'NNP', 'DT', 'NNS', 'JJ', 'COMMA', 'CD', '.', 'VBD', 'RB','VB', 'CC', 'VBN', 'VBZ', \n",
    "          'VBG', 'TO', 'PRP', 'VBP', 'POS', 'PRP$','MD', '$', '``', \"''\", 'WDT', ':', 'JJR', 'RP', 'RBR', \n",
    "          'WP', 'NNPS','JJS', ')', '(', 'EX', 'RBS', 'WRB', '-', 'UH', 'WP$', 'PDT', '/', '#', 'LS', 'SYM', 'FW', 'AUX']\n",
    "for pos in POS_ls:\n",
    "    POS_id[pos] = cnt\n",
    "    cnt += 1\n",
    "\n",
    "# map BIO to BIO_id\n",
    "cnt = 0\n",
    "BIO_id = {}\n",
    "BIO_ls = ['O', 'B-NP', 'I-NP', 'B-PP', 'B-ADVP', 'B-ADJP', 'B-SBAR', 'B-CONJP',\n",
    "       'I-ADJP', 'I-PP', 'I-ADVP', 'I-CONJP', 'B-INTJ', 'I-SBAR', 'B-LST',\n",
    "       'B-VP', 'B-PRT', 'I-INTJ', 'I-VP']\n",
    "for bio in BIO_ls:\n",
    "    BIO_id[bio] = cnt\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2116af7-acbd-4fdf-ab5e-80ef11677cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'BIO', 'POS', 'label', 'id'],\n",
       "        num_rows: 84169\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'BIO', 'POS', 'label', 'id'],\n",
       "        num_rows: 3235\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'BIO', 'POS', 'label', 'id'],\n",
       "        num_rows: 5382\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map label to BIO_id\n",
    "import glob\n",
    "Label_id = {\"ARG0\":0,\"ARG1\":1,\"ARG2\":2,\"PRED\":3,\"SUPPORT\":4}\n",
    "def mapLabel(label):\n",
    "    return Label_id[label] if label in Label_id else 5\n",
    "\n",
    "# build datasets\n",
    "def condense_df(target):\n",
    "    result = []\n",
    "    for file in glob.glob(\"nombank_train_dev_test/\"+target):\n",
    "\n",
    "        with open(file, 'r') as file:\n",
    "\n",
    "            ls = [i.split('\\t') for i in file.read().split('\\n')]\n",
    "\n",
    "            result.append(pd.DataFrame(ls))\n",
    "\n",
    "    df = pd.concat(result)\n",
    "\n",
    "    df['id'] = df.index\n",
    "    df[0].replace('', None, inplace=True)\n",
    "    df.dropna(axis=0, subset = [4], inplace = True)\n",
    "    df['BIO'] = df[2].map(BIO_id)\n",
    "    df['POS'] = df[1].map(POS_id)\n",
    "    df['label'] = df[5].map(mapLabel)\n",
    "    df['id'] = df[4].map(int)\n",
    "    df.drop(columns = [1, 2, 3, 4, 5, 6], inplace = True)\n",
    "    condense = df.groupby('id').apply(lambda x: [list(x[0]),list(x['POS']), list(x['BIO']), list(x['label'])]).apply(pd.Series)\n",
    "    condense.columns =['tokens','BIO', 'POS','label']\n",
    "    return condense\n",
    "\n",
    "\n",
    "train = Dataset.from_pandas(condense_df(\"*.train\"))\n",
    "eval_ = Dataset.from_pandas(condense_df(\"*.dev\"))\n",
    "test = Dataset.from_pandas(condense_df(\"*.test\"))\n",
    "\n",
    "\n",
    "datasets = DatasetDict({\"train\": train, \"validation\":eval_, \"test\":test})\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad2a9519-741f-4eb3-a074-2af5e116fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9322db92-e1ac-4c82-9f7c-86d7f9b75943",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89377d5c4e5f4084a966598e12ba5dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd94b1ddd044fba87ed5d6a8f15a143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09d5fd3b3e7436b8ccd15e7dcb9da43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "NonePOS = len(POS_ls)\n",
    "NoneBIO = len(BIO_ls)\n",
    "def align_labels_with_tokens(labels, POSs, BIOs, word_ids):\n",
    "    new_labels = []\n",
    "    POS_labels = []\n",
    "    BIO_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if not word_id:\n",
    "            new_labels.append(-100)\n",
    "            POS_labels.append(NonePOS)\n",
    "            BIO_labels.append(NoneBIO)\n",
    "        else:\n",
    "            if word_id != current_word:# Start of a new word!\n",
    "                current_word = word_id       \n",
    "            new_labels.append(labels[word_id])\n",
    "            POS_labels.append(POSs[word_id])\n",
    "            BIO_labels.append(BIOs[word_id])\n",
    "\n",
    "    return new_labels, POS_labels, BIO_labels\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, padding=True, max_length = 48)\n",
    "    all_BIO = examples[\"BIO\"]\n",
    "    all_POS = examples[\"POS\"]\n",
    "    all_labels = examples[\"label\"]\n",
    "    \n",
    "    new_labels = []\n",
    "    POS_labels = []\n",
    "    BIO_labels = []\n",
    "    for i in range(len(all_labels)):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_label, POS_label, BIO_label = align_labels_with_tokens(all_labels[i], all_POS[i], all_BIO[i], word_ids)\n",
    "        new_labels.append(new_label)\n",
    "        POS_labels.append(POS_label)\n",
    "        BIO_labels.append(BIO_label)\n",
    "    \n",
    "    curLen = len(BIO_labels)\n",
    "\n",
    "    tokenized_inputs[\"BIOL\"] = BIO_labels\n",
    "    tokenized_inputs[\"POSL\"] = POS_labels\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    \n",
    "\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns = datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa27677f-cf86-4752-b389-505d32e45f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. train while evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d03d8e7-778d-4d0f-b6d2-d5c940a71b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "#small_train = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(64*8))\n",
    "#small_eval = tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(64*8))\n",
    "#small_test = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(64*8))\n",
    "\n",
    "\n",
    "BatchSize = 64\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, batch_size=BatchSize, collate_fn=data_collator)##small_train\n",
    "eval_dataloader = DataLoader(tokenized_datasets[\"validation\"], batch_size=BatchSize, collate_fn=data_collator)##small_eval\n",
    "test_dataloader = DataLoader(tokenized_datasets[\"test\"], batch_size=BatchSize, collate_fn=data_collator)##small_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8c02d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\"ARG0\", \"ARG1\", \"ARG2\", \"PRED\", \"SUPPORT\", \"None\"]\n",
    "POS_len = len(POS_ls)\n",
    "BIO_len = len(BIO_ls)\n",
    "feature_dim = 866"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61d8796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "import torch.nn as nn\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self,checkpoint,num_labels): \n",
    "        super(CustomModel,self).__init__() \n",
    "        self.num_labels = num_labels \n",
    "\n",
    "        #Load Model with given checkpoint and extract its body\n",
    "        self.model = AutoModel.from_pretrained(checkpoint,config=AutoConfig.from_pretrained(checkpoint, output_attentions=True,output_hidden_states=True))\n",
    "        self.dropout = nn.Dropout(0.1) \n",
    "        self.classifier = nn.Linear(feature_dim,num_labels) # load and initialize weights\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, labels, POS, BIO):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        sequence_output = self.dropout(outputs[0]) #outputs[0]=last hidden state\n",
    "        \n",
    "        POS_f = torch.eye(POS_len+1)[POS]\n",
    "        BIO_f = torch.eye(POS_len+1)[BIO]\n",
    "        \n",
    "        sequence_output = torch.cat((sequence_output,POS_f, BIO_f),2)\n",
    "        #print(sequence_output.size())\n",
    "        logits = self.classifier(sequence_output[:,:,:].view(-1, feature_dim)) # calculate losses\n",
    "        self.logits = logits\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states,attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a0c4d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/jiajingchen/miniforge3/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|                                                  | 0/3948 [00:00<?, ?it/s]\n",
      "  0%|                                                   | 0/153 [00:00<?, ?it/s]\u001b[AYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  0%|                                        | 2/3948 [00:05<2:55:48,  2.67s/it]\n",
      " 33%|███████████▋                       | 1316/3948 [6:12:34<1:27:08,  1.99s/it]\u001b[A/Users/jiajingchen/miniforge3/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: None seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/jiajingchen/miniforge3/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRED seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/jiajingchen/miniforge3/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SUPPORT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/jiajingchen/miniforge3/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ARG1 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/jiajingchen/miniforge3/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ARG2 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/jiajingchen/miniforge3/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ARG0 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/jiajingchen/miniforge3/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "  1%|▏                                 | 1/153 [6:12:35<943:53:05, 22355.17s/it]\u001b[A\n",
      "  1%|▍                                  | 2/153 [6:12:36<386:07:20, 9205.57s/it]\u001b[A\n",
      "  2%|▋                                  | 3/153 [6:12:36<208:26:13, 5002.49s/it]\u001b[A\n",
      "  3%|▉                                  | 4/153 [6:12:37<125:19:15, 3027.89s/it]\u001b[A\n",
      "  3%|█▏                                  | 5/153 [6:12:38<79:36:14, 1936.31s/it]\u001b[A\n",
      "  4%|█▍                                  | 6/153 [6:12:39<52:11:42, 1278.25s/it]\u001b[A\n",
      "  5%|█▋                                   | 7/153 [6:12:40<34:54:12, 860.63s/it]\u001b[A\n",
      "  5%|█▉                                   | 8/153 [6:12:41<23:38:22, 586.91s/it]\u001b[A\n",
      "  6%|██▏                                  | 9/153 [6:12:41<16:08:52, 403.70s/it]\u001b[A\n",
      "  7%|██▎                                 | 10/153 [6:12:42<11:05:44, 279.33s/it]\u001b[A\n",
      "  7%|██▋                                  | 11/153 [6:12:43<7:39:22, 194.10s/it]\u001b[A\n",
      "  8%|██▉                                  | 12/153 [6:12:44<5:17:58, 135.31s/it]\u001b[A\n",
      "  8%|███▏                                  | 13/153 [6:12:45<3:40:40, 94.57s/it]\u001b[A\n",
      "  9%|███▍                                  | 14/153 [6:12:46<2:33:30, 66.26s/it]\u001b[A\n",
      " 10%|███▋                                  | 15/153 [6:12:46<1:47:02, 46.54s/it]\u001b[A\n",
      " 10%|███▉                                  | 16/153 [6:12:47<1:14:51, 32.78s/it]\u001b[A\n",
      " 11%|████▍                                   | 17/153 [6:12:48<52:32, 23.18s/it]\u001b[A\n",
      " 12%|████▋                                   | 18/153 [6:12:49<37:02, 16.47s/it]\u001b[A\n",
      " 12%|████▉                                   | 19/153 [6:12:50<26:17, 11.77s/it]\u001b[A\n",
      " 13%|█████▏                                  | 20/153 [6:12:51<18:49,  8.49s/it]\u001b[A\n",
      " 14%|█████▍                                  | 21/153 [6:12:51<13:37,  6.19s/it]\u001b[A\n",
      " 14%|█████▊                                  | 22/153 [6:12:52<10:00,  4.58s/it]\u001b[A\n",
      " 15%|██████                                  | 23/153 [6:12:53<07:31,  3.47s/it]\u001b[A\n",
      " 16%|██████▎                                 | 24/153 [6:12:54<05:49,  2.71s/it]\u001b[A\n",
      " 16%|██████▌                                 | 25/153 [6:12:55<04:38,  2.17s/it]\u001b[A\n",
      " 17%|██████▊                                 | 26/153 [6:12:56<03:46,  1.78s/it]\u001b[A\n",
      " 18%|███████                                 | 27/153 [6:12:57<03:09,  1.50s/it]\u001b[A\n",
      " 18%|███████▎                                | 28/153 [6:12:58<02:43,  1.31s/it]\u001b[A\n",
      " 19%|███████▌                                | 29/153 [6:12:58<02:24,  1.17s/it]\u001b[A\n",
      " 20%|███████▊                                | 30/153 [6:12:59<02:12,  1.07s/it]\u001b[A\n",
      " 20%|████████                                | 31/153 [6:13:00<02:02,  1.01s/it]\u001b[A\n",
      " 21%|████████▎                               | 32/153 [6:13:01<01:56,  1.04it/s]\u001b[A\n",
      " 22%|████████▋                               | 33/153 [6:13:02<01:51,  1.08it/s]\u001b[A\n",
      " 22%|████████▉                               | 34/153 [6:13:03<01:48,  1.10it/s]\u001b[A\n",
      " 23%|█████████▏                              | 35/153 [6:13:04<01:44,  1.13it/s]\u001b[A\n",
      " 24%|█████████▍                              | 36/153 [6:13:04<01:42,  1.14it/s]\u001b[A\n",
      " 24%|█████████▋                              | 37/153 [6:13:05<01:40,  1.15it/s]\u001b[A\n",
      " 25%|█████████▉                              | 38/153 [6:13:06<01:39,  1.15it/s]\u001b[A\n",
      " 25%|██████████▏                             | 39/153 [6:13:07<01:38,  1.16it/s]\u001b[A\n",
      " 26%|██████████▍                             | 40/153 [6:13:08<01:37,  1.16it/s]\u001b[A\n",
      " 27%|██████████▋                             | 41/153 [6:13:09<01:36,  1.17it/s]\u001b[A\n",
      " 27%|██████████▉                             | 42/153 [6:13:10<01:34,  1.17it/s]\u001b[A\n",
      " 28%|███████████▏                            | 43/153 [6:13:10<01:33,  1.17it/s]\u001b[A\n",
      " 29%|███████████▌                            | 44/153 [6:13:11<01:32,  1.18it/s]\u001b[A\n",
      " 29%|███████████▊                            | 45/153 [6:13:12<01:31,  1.18it/s]\u001b[A\n",
      " 30%|████████████                            | 46/153 [6:13:13<01:30,  1.18it/s]\u001b[A\n",
      " 31%|████████████▎                           | 47/153 [6:13:14<01:29,  1.19it/s]\u001b[A\n",
      " 31%|████████████▌                           | 48/153 [6:13:15<01:27,  1.20it/s]\u001b[A\n",
      " 32%|████████████▊                           | 49/153 [6:13:15<01:26,  1.21it/s]\u001b[A\n",
      " 33%|█████████████                           | 50/153 [6:13:16<01:25,  1.21it/s]\u001b[A"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[64, -1]' is invalid for input of size 1680",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5d/l9sqzgw56mv0n0wg3kfxh0840000gn/T/ipykernel_12483/820149666.py\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mtrue_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mreshaped_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBatchSize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         true_predictions = [\n\u001b[1;32m     52\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mlabel_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[64, -1]' is invalid for input of size 1680"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW,get_scheduler\n",
    "from datasets import load_metric\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "num_epochs = 3\n",
    "model_cc = CustomModel(checkpoint=model_checkpoint,num_labels=len(label_names))#.cuda()\n",
    "optimizer = AdamW(model_cc.parameters(), lr=2e-5)\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "progress_bar_train = tqdm(range(num_training_steps),miniters=2)\n",
    "progress_bar_eval = tqdm(range(num_epochs * len(eval_dataloader)),miniters=2)\n",
    "f1_best = 0\n",
    "resume_flag = False\n",
    "best_net = None\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    if resume_flag:\n",
    "        model_cc.load_state_dict(torch.load(\"sstcls_best.dat\"))\n",
    "    model_cc.train()\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v for k, v in batch.items()}#.cuda()\n",
    "        outputs = model_cc(batch['input_ids'], token_type_ids=batch['token_type_ids'], attention_mask=batch['attention_mask'],labels=batch['labels'], POS=batch['POSL'], BIO=batch['BIOL'])\n",
    "      \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar_train.update(1)\n",
    "    model_cc.eval()\n",
    "    f1_now = []\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v for k, v in batch.items()}#.cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs = model_cc(batch['input_ids'], token_type_ids=batch['token_type_ids'], attention_mask=batch['attention_mask'],labels=batch['labels'], POS=batch['POSL'], BIO=batch['BIOL'])\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        true_labels = [[label_names[l] for l in label if l != -100] for label in batch[\"labels\"]]\n",
    "        \n",
    "        try:\n",
    "            reshaped_predictions = torch.reshape(predictions, (BatchSize,-1))\n",
    "            true_predictions = [\n",
    "                [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "                for prediction, label in zip(reshaped_predictions, batch[\"labels\"])\n",
    "            ]\n",
    "\n",
    "            all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "            metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "            progress_bar_eval.update(1)\n",
    "            f1_now.append(all_metrics[\"overall_f1\"])\n",
    "        except:\n",
    "            print(\"***** exception ***** \",predictions)\n",
    "            f1_now.append(0)\n",
    "        \n",
    "    if np.mean(f1_now) > f1_best or not best_net:\n",
    "        torch.save(model_cc.state_dict(), 'sstcls_best.dat')\n",
    "        f1_best = np.mean(f1_now)\n",
    "        print(\"the best f1 is now: \"+ str(np.mean(f1_now)))\n",
    "        best_net = model_cc\n",
    "        \n",
    "\n",
    "    print(metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dea989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, dataloader, gpu):\n",
    "    net.eval()\n",
    "\n",
    "    f1_now = []\n",
    "    precision_now = []\n",
    "    recall_now = []\n",
    "    accuracy_now = []\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v for k, v in batch.items()}#.cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs = model_cc(batch['input_ids'], token_type_ids=batch['token_type_ids'], attention_mask=batch['attention_mask'], labels=batch['labels'], POS=batch['POSL'], BIO=batch['BIOL'])\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        true_labels = [[label_names[l] for l in label if l != -100] for label in batch[\"labels\"]]\n",
    "        \n",
    "        try:\n",
    "            reshaped_predictions = torch.reshape(predictions, (BatchSize,-1))\n",
    "            true_predictions = [\n",
    "                [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "                for prediction, label in zip(reshaped_predictions, batch[\"labels\"])\n",
    "            ]\n",
    "\n",
    "            all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "            metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "            progress_bar_eval.update(1)\n",
    "            f1_now.append(all_metrics[\"overall_f1\"])\n",
    "            precision_now.append(all_metrics[\"overall_precision\"])\n",
    "            recall_now.append(all_metrics[\"overall_recall\"])\n",
    "            accuracy_now.append(all_metrics[\"overall_accuracy\"])\n",
    "        \n",
    "        except:\n",
    "            print(\"***** exception ***** \",predictions)\n",
    "            f1_now.append(0)\n",
    "            precision_now.append(0)\n",
    "            recall_now.append(0)\n",
    "            accuracy_now.append(0)\n",
    "\n",
    "\n",
    "    return np.mean(f1_now), np.mean(precision_now), np.mean(recall_now), np.mean(accuracy_now),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca124a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict(best_net, test_dataloader, 0)\n",
    "print(\"test_f1_score:::\"+str(preds[0]))\n",
    "print(\"test_precision_score:::\"+str(preds[1]))\n",
    "print(\"test_recall_score:::\"+str(preds[2]))\n",
    "print(\"test_accuracy_score:::\"+str(preds[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da1c4de-27f1-4e49-be87-655003d4ad13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
