{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "678a7b49-66d2-47ee-8c00-d29cacb92784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. build our own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff3c14f-c9d7-4faf-a935-99393ce2d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# map POS to POS_id\n",
    "cnt = 0\n",
    "POS_id = {}\n",
    "POS_ls = ['NN', 'IN', 'NNP', 'DT', 'NNS', 'JJ', 'COMMA', 'CD', '.', 'VBD', 'RB','VB', 'CC', 'VBN', 'VBZ', \n",
    "          'VBG', 'TO', 'PRP', 'VBP', 'POS', 'PRP$','MD', '$', '``', \"''\", 'WDT', ':', 'JJR', 'RP', 'RBR', \n",
    "          'WP', 'NNPS','JJS', ')', '(', 'EX', 'RBS', 'WRB', '-', 'UH', 'WP$', 'PDT', '/', '#', 'LS', 'SYM', 'FW', 'AUX']\n",
    "for pos in POS_ls:\n",
    "    POS_id[pos] = cnt\n",
    "    cnt += 1\n",
    "\n",
    "# map BIO to BIO_id\n",
    "cnt = 0\n",
    "BIO_id = {}\n",
    "BIO_ls = ['O', 'B-NP', 'I-NP', 'B-PP', 'B-ADVP', 'B-ADJP', 'B-SBAR', 'B-CONJP',\n",
    "       'I-ADJP', 'I-PP', 'I-ADVP', 'I-CONJP', 'B-INTJ', 'I-SBAR', 'B-LST',\n",
    "       'B-VP', 'B-PRT', 'I-INTJ', 'I-VP']\n",
    "for bio in BIO_ls:\n",
    "    BIO_id[bio] = cnt\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2116af7-acbd-4fdf-ab5e-80ef11677cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'BIO', 'POS', 'label', 'id'],\n",
       "        num_rows: 84169\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'BIO', 'POS', 'label', 'id'],\n",
       "        num_rows: 3235\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'BIO', 'POS', 'label', 'id'],\n",
       "        num_rows: 5382\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map label to BIO_id\n",
    "import glob\n",
    "Label_id = {\"ARG0\":0,\"ARG1\":1,\"ARG2\":2,\"PRED\":3,\"SUPPORT\":4}\n",
    "def mapLabel(label):\n",
    "    return Label_id[label] if label in Label_id else 5\n",
    "\n",
    "# build datasets\n",
    "def condense_df(target):\n",
    "    result = []\n",
    "    for file in glob.glob(\"nombank_train_dev_test/\"+target):\n",
    "\n",
    "        with open(file, 'r') as file:\n",
    "\n",
    "            ls = [i.split('\\t') for i in file.read().split('\\n')]\n",
    "\n",
    "            result.append(pd.DataFrame(ls))\n",
    "\n",
    "    df = pd.concat(result)\n",
    "\n",
    "    df['id'] = df.index\n",
    "    df[0].replace('', None, inplace=True)\n",
    "    df.dropna(axis=0, subset = [4], inplace = True)\n",
    "    df['BIO'] = df[2].map(BIO_id)\n",
    "    df['POS'] = df[1].map(POS_id)\n",
    "    df['label'] = df[5].map(mapLabel)\n",
    "    df['id'] = df[4].map(int)\n",
    "    df.drop(columns = [1, 2, 3, 4, 5, 6], inplace = True)\n",
    "    condense = df.groupby('id').apply(lambda x: [list(x[0]),list(x['POS']), list(x['BIO']), list(x['label'])]).apply(pd.Series)\n",
    "    condense.columns =['tokens','BIO', 'POS','label']\n",
    "    return condense\n",
    "\n",
    "\n",
    "train = Dataset.from_pandas(condense_df(\"*.train\"))\n",
    "eval_ = Dataset.from_pandas(condense_df(\"*.dev\"))\n",
    "test = Dataset.from_pandas(condense_df(\"*.test\"))\n",
    "\n",
    "\n",
    "datasets = DatasetDict({\"train\": train, \"validation\":eval_, \"test\":test})\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad2a9519-741f-4eb3-a074-2af5e116fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9322db92-e1ac-4c82-9f7c-86d7f9b75943",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb0d2e6f1b340ef897f52ea49f3c3aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2949ac9c010c42f89ee1176960243973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd68d0f9f4e4962a379972e70b898e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "NonePOS = len(POS_ls)\n",
    "NoneBIO = len(BIO_ls)\n",
    "def align_labels_with_tokens(labels, POSs, BIOs, word_ids):\n",
    "    new_labels = []\n",
    "    POS_labels = []\n",
    "    BIO_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if not word_id:\n",
    "            new_labels.append(-100)\n",
    "            POS_labels.append(NonePOS)\n",
    "            BIO_labels.append(NoneBIO)\n",
    "        else:\n",
    "            if word_id != current_word:# Start of a new word!\n",
    "                current_word = word_id       \n",
    "            new_labels.append(labels[word_id])\n",
    "            POS_labels.append(POSs[word_id])\n",
    "            BIO_labels.append(BIOs[word_id])\n",
    "\n",
    "    return new_labels, POS_labels, BIO_labels\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, padding=True, max_length = 48)\n",
    "    all_BIO = examples[\"BIO\"]\n",
    "    all_POS = examples[\"POS\"]\n",
    "    all_labels = examples[\"label\"]\n",
    "    \n",
    "    new_labels = []\n",
    "    POS_labels = []\n",
    "    BIO_labels = []\n",
    "    for i in range(len(all_labels)):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_label, POS_label, BIO_label = align_labels_with_tokens(all_labels[i], all_POS[i], all_BIO[i], word_ids)\n",
    "        new_labels.append(new_label)\n",
    "        POS_labels.append(POS_label)\n",
    "        BIO_labels.append(BIO_label)\n",
    "    \n",
    "    curLen = len(BIO_labels)\n",
    "\n",
    "    tokenized_inputs[\"BIOL\"] = BIO_labels\n",
    "    tokenized_inputs[\"POSL\"] = POS_labels\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    \n",
    "\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns = datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa27677f-cf86-4752-b389-505d32e45f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. train while evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d03d8e7-778d-4d0f-b6d2-d5c940a71b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "#small_train = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(64*8))\n",
    "#small_eval = tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(64*8))\n",
    "#small_test = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(64*8))\n",
    "\n",
    "\n",
    "BatchSize = 64\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, batch_size=BatchSize, collate_fn=data_collator)##small_train\n",
    "eval_dataloader = DataLoader(tokenized_datasets[\"validation\"], batch_size=BatchSize, collate_fn=data_collator)##small_eval\n",
    "test_dataloader = DataLoader(tokenized_datasets[\"test\"], batch_size=BatchSize, collate_fn=data_collator)##small_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c02d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\"ARG0\", \"ARG1\", \"ARG2\", \"PRED\", \"SUPPORT\", \"None\"]\n",
    "POS_len = len(POS_ls)\n",
    "BIO_len = len(BIO_ls)\n",
    "feature_dim = 866"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d8796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "import torch.nn as nn\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self,checkpoint,num_labels): \n",
    "        super(CustomModel,self).__init__() \n",
    "        self.num_labels = num_labels \n",
    "\n",
    "        #Load Model with given checkpoint and extract its body\n",
    "        self.model = AutoModel.from_pretrained(checkpoint,config=AutoConfig.from_pretrained(checkpoint, output_attentions=True,output_hidden_states=True))\n",
    "        self.dropout = nn.Dropout(0.1) \n",
    "        self.classifier = nn.Linear(feature_dim,num_labels) # load and initialize weights\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, labels, POS, BIO):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        sequence_output = self.dropout(outputs[0]) #outputs[0]=last hidden state\n",
    "        \n",
    "        POS_f = torch.eye(POS_len+1)[POS]\n",
    "        BIO_f = torch.eye(POS_len+1)[BIO]\n",
    "        \n",
    "        sequence_output = torch.cat((sequence_output,POS_f, BIO_f),2)\n",
    "        print(sequence_output.size())\n",
    "        logits = self.classifier(sequence_output[:,:,:].view(-1, feature_dim)) # calculate losses\n",
    "        self.logits = logits\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states,attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0c4d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW,get_scheduler\n",
    "from datasets import load_metric\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "num_epochs = 3\n",
    "model_cc = CustomModel(checkpoint=model_checkpoint,num_labels=len(label_names))#.cuda()\n",
    "optimizer = AdamW(model_cc.parameters(), lr=2e-5)\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "progress_bar_train = tqdm(range(num_training_steps),miniters=2)\n",
    "progress_bar_eval = tqdm(range(num_epochs * len(eval_dataloader)),miniters=2)\n",
    "f1_best = 0\n",
    "resume_flag = False\n",
    "best_net = None\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    if resume_flag:\n",
    "        model_cc.load_state_dict(torch.load(\"sstcls_best.dat\"))\n",
    "    model_cc.train()\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v for k, v in batch.items()}#.cuda()\n",
    "        outputs = model_cc(batch['input_ids'], token_type_ids=batch['token_type_ids'], attention_mask=batch['attention_mask'],labels=batch['labels'], POS=batch['POSL'], BIO=batch['BIOL'])\n",
    "      \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar_train.update(1)\n",
    "    model_cc.eval()\n",
    "    f1_now = []\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v for k, v in batch.items()}#.cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs = model_cc(batch['input_ids'], token_type_ids=batch['token_type_ids'], attention_mask=batch['attention_mask'],labels=batch['labels'], POS=batch['POSL'], BIO=batch['BIOL'])\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        true_labels = [[label_names[l] for l in label if l != -100] for label in batch[\"labels\"]]\n",
    "        \n",
    "        reshaped_predictions = torch.reshape(predictions, (BatchSize,-1))\n",
    "        true_predictions = [\n",
    "            [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(reshaped_predictions, batch[\"labels\"])\n",
    "        ]\n",
    "        \n",
    "        all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "        progress_bar_eval.update(1)\n",
    "        f1_now.append(all_metrics[\"overall_f1\"])\n",
    "        \n",
    "    if np.mean(f1_now) > f1_best or not best_net:\n",
    "        torch.save(model_cc.state_dict(), 'sstcls_best.dat')\n",
    "        f1_best = np.mean(f1_now)\n",
    "        print(\"the best f1 is now: \"+ str(np.mean(f1_now)))\n",
    "        best_net = model_cc\n",
    "        \n",
    "\n",
    "    print(metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dea989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, dataloader, gpu):\n",
    "    net.eval()\n",
    "\n",
    "    f1_now = []\n",
    "    precision_now = []\n",
    "    recall_now = []\n",
    "    accuracy_now = []\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v for k, v in batch.items()}#.cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs = model_cc(batch['input_ids'], token_type_ids=batch['token_type_ids'], attention_mask=batch['attention_mask'], labels=batch['labels'], POS=batch['POSL'], BIO=batch['BIOL'])\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        true_labels = [[label_names[l] for l in label if l != -100] for label in batch[\"labels\"]]\n",
    "        \n",
    "        reshaped_predictions = torch.reshape(predictions, (BatchSize,-1))\n",
    "        true_predictions = [\n",
    "            [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(reshaped_predictions, batch[\"labels\"])\n",
    "        ]\n",
    "        \n",
    "        all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "        progress_bar_eval.update(1)\n",
    "        f1_now.append(all_metrics[\"overall_f1\"])\n",
    "        precision_now.append(all_metrics[\"overall_precision\"])\n",
    "        recall_now.append(all_metrics[\"overall_recall\"])\n",
    "        accuracy_now.append(all_metrics[\"overall_accuracy\"])\n",
    "\n",
    "\n",
    "    return np.mean(f1_now), np.mean(precision_now), np.mean(recall_now), np.mean(accuracy_now),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca124a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict(best_net, test_dataloader, 0)\n",
    "print(\"test_f1_score:::\"+str(preds[0]))\n",
    "print(\"test_precision_score:::\"+str(preds[1]))\n",
    "print(\"test_recall_score:::\"+str(preds[2]))\n",
    "print(\"test_accuracy_score:::\"+str(preds[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da1c4de-27f1-4e49-be87-655003d4ad13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
