{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "678a7b49-66d2-47ee-8c00-d29cacb92784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. build our own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff3c14f-c9d7-4faf-a935-99393ce2d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# map POS to POS_id\n",
    "cnt = 0\n",
    "POS_id = {}\n",
    "POS_ls = ['NN', 'IN', 'NNP', 'DT', 'NNS', 'JJ', 'COMMA', 'CD', '.', 'VBD', 'RB','VB', 'CC', 'VBN', 'VBZ', \n",
    "          'VBG', 'TO', 'PRP', 'VBP', 'POS', 'PRP$','MD', '$', '``', \"''\", 'WDT', ':', 'JJR', 'RP', 'RBR', \n",
    "          'WP', 'NNPS','JJS', ')', '(', 'EX', 'RBS', 'WRB', '-', 'UH', 'WP$', 'PDT', '/', '#', 'LS', 'SYM', 'FW', 'AUX']\n",
    "for pos in POS_ls:\n",
    "    POS_id[pos] = cnt\n",
    "    cnt += 1\n",
    "\n",
    "# map BIO to BIO_id\n",
    "cnt = 0\n",
    "BIO_id = {}\n",
    "BIO_ls = ['O', 'B-NP', 'I-NP', 'B-PP', 'B-ADVP', 'B-ADJP', 'B-SBAR', 'B-CONJP',\n",
    "       'I-ADJP', 'I-PP', 'I-ADVP', 'I-CONJP', 'B-INTJ', 'I-SBAR', 'B-LST',\n",
    "       'B-VP', 'B-PRT', 'I-INTJ', 'I-VP']\n",
    "for bio in BIO_ls:\n",
    "    BIO_id[bio] = cnt\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2116af7-acbd-4fdf-ab5e-80ef11677cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'BIO', 'POS', 'label', 'id'],\n",
       "        num_rows: 84169\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'BIO', 'POS', 'label', 'id'],\n",
       "        num_rows: 3235\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'BIO', 'POS', 'label', 'id'],\n",
       "        num_rows: 5382\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map label to BIO_id\n",
    "import glob\n",
    "Label_id = {\"ARG0\":0,\"ARG1\":1,\"ARG2\":2,\"PRED\":3,\"SUPPORT\":4}\n",
    "def mapLabel(label):\n",
    "    return Label_id[label] if label in Label_id else 5\n",
    "\n",
    "# build datasets\n",
    "def condense_df(target):\n",
    "    result = []\n",
    "    for file in glob.glob(\"nombank_train_dev_test/\"+target):\n",
    "\n",
    "        with open(file, 'r') as file:\n",
    "\n",
    "            ls = [i.split('\\t') for i in file.read().split('\\n')]\n",
    "\n",
    "            result.append(pd.DataFrame(ls))\n",
    "\n",
    "    df = pd.concat(result)\n",
    "\n",
    "    df['id'] = df.index\n",
    "    df[0].replace('', None, inplace=True)\n",
    "    df.dropna(axis=0, subset = [4], inplace = True)\n",
    "    df['BIO'] = df[2].map(BIO_id)\n",
    "    df['POS'] = df[1].map(POS_id)\n",
    "    df['label'] = df[5].map(mapLabel)\n",
    "    df['id'] = df[4].map(int)\n",
    "    df.drop(columns = [1, 2, 3, 4, 5, 6], inplace = True)\n",
    "    condense = df.groupby('id').apply(lambda x: [list(x[0]),list(x['POS']), list(x['BIO']), list(x['label'])]).apply(pd.Series)\n",
    "    condense.columns =['tokens','BIO', 'POS','label']\n",
    "    return condense\n",
    "\n",
    "\n",
    "train = Dataset.from_pandas(condense_df(\"*.train\"))\n",
    "eval_ = Dataset.from_pandas(condense_df(\"*.dev\"))\n",
    "test = Dataset.from_pandas(condense_df(\"*.test\"))\n",
    "\n",
    "\n",
    "datasets = DatasetDict({\"train\": train, \"validation\":eval_, \"test\":test})\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad2a9519-741f-4eb3-a074-2af5e116fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9322db92-e1ac-4c82-9f7c-86d7f9b75943",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ba18ccd2ca440a8fe0573f0c35f68f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c93633f9749414ea48a671c768c6455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b3dc2369d1646b58f20d180fc78f877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "NonePOS = len(POS_ls)\n",
    "NoneBIO = len(BIO_ls)\n",
    "def align_labels_with_tokens(labels, POSs, BIOs, word_ids):\n",
    "    new_labels = []\n",
    "    POS_labels = []\n",
    "    BIO_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if not word_id:\n",
    "            new_labels.append(-100)\n",
    "            POS_labels.append(NonePOS)\n",
    "            BIO_labels.append(NoneBIO)\n",
    "        else:\n",
    "            if word_id != current_word:# Start of a new word!\n",
    "                current_word = word_id       \n",
    "            new_labels.append(labels[word_id])\n",
    "            POS_labels.append(POSs[word_id])\n",
    "            BIO_labels.append(BIOs[word_id])\n",
    "\n",
    "    return new_labels, POS_labels, BIO_labels\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, padding=True, max_length = 48)\n",
    "    all_BIO = examples[\"BIO\"]\n",
    "    all_POS = examples[\"POS\"]\n",
    "    all_labels = examples[\"label\"]\n",
    "    \n",
    "    new_labels = []\n",
    "    POS_labels = []\n",
    "    BIO_labels = []\n",
    "    for i in range(len(all_labels)):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_label, POS_label, BIO_label = align_labels_with_tokens(all_labels[i], all_POS[i], all_BIO[i], word_ids)\n",
    "        new_labels.append(new_label)\n",
    "        POS_labels.append(POS_label)\n",
    "        BIO_labels.append(BIO_label)\n",
    "    \n",
    "    curLen = len(BIO_labels)\n",
    "\n",
    "    tokenized_inputs[\"BIOL\"] = BIO_labels\n",
    "    tokenized_inputs[\"POSL\"] = POS_labels\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    \n",
    "\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns = datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa27677f-cf86-4752-b389-505d32e45f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. train while evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d03d8e7-778d-4d0f-b6d2-d5c940a71b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "#small_train = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(64*8))\n",
    "#small_eval = tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(64*8))\n",
    "#small_test = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(64*8))\n",
    "\n",
    "\n",
    "BatchSize = 64\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, batch_size=BatchSize, collate_fn=data_collator)##small_train\n",
    "eval_dataloader = DataLoader(tokenized_datasets[\"validation\"], batch_size=BatchSize, collate_fn=data_collator)##small_eval\n",
    "test_dataloader = DataLoader(tokenized_datasets[\"test\"], batch_size=BatchSize, collate_fn=data_collator)##small_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8c02d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\"ARG0\", \"ARG1\", \"ARG2\", \"PRED\", \"SUPPORT\", \"None\"]\n",
    "POS_len = len(POS_ls)\n",
    "BIO_len = len(BIO_ls)\n",
    "feature_dim = 866"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61d8796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "import torch.nn as nn\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self,checkpoint,num_labels): \n",
    "        super(CustomModel,self).__init__() \n",
    "        self.num_labels = num_labels \n",
    "\n",
    "        #Load Model with given checkpoint and extract its body\n",
    "        self.model = AutoModel.from_pretrained(checkpoint,config=AutoConfig.from_pretrained(checkpoint, output_attentions=True,output_hidden_states=True))\n",
    "        self.dropout = nn.Dropout(0.1) \n",
    "        self.classifier = nn.Linear(feature_dim,num_labels) # load and initialize weights\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, labels, POS, BIO):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        sequence_output = self.dropout(outputs[0]) #outputs[0]=last hidden state\n",
    "        \n",
    "        POS_f = torch.eye(POS_len+1)[POS]\n",
    "        BIO_f = torch.eye(POS_len+1)[BIO]\n",
    "        \n",
    "        sequence_output = torch.cat((sequence_output,POS_f, BIO_f),2)\n",
    "        #print(sequence_output.size())\n",
    "        logits = self.classifier(sequence_output[:,:,:].view(-1, feature_dim)) # calculate losses\n",
    "        self.logits = logits\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states,attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a0c4d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/jiajingchen/miniforge3/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|                                                  | 0/3948 [00:00<?, ?it/s]\n",
      "  0%|                                                   | 0/153 [00:00<?, ?it/s]\u001b[AYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  0%|                                        | 2/3948 [00:06<3:25:52,  3.13s/it]\n",
      " 33%|████████████▎                        | 1316/3948 [59:30<1:43:20,  2.36s/it]\u001b[A/Users/jiajingchen/miniforge3/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: None seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/jiajingchen/miniforge3/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRED seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/jiajingchen/miniforge3/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SUPPORT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/jiajingchen/miniforge3/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ARG1 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/jiajingchen/miniforge3/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ARG2 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/jiajingchen/miniforge3/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ARG0 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/jiajingchen/miniforge3/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "  1%|▏                                    | 1/153 [59:31<150:46:51, 3571.13s/it]\u001b[A\n",
      "  1%|▍                                     | 2/153 [59:31<61:41:56, 1470.97s/it]\u001b[A\n",
      "  2%|▊                                      | 3/153 [59:32<33:19:14, 799.69s/it]\u001b[A\n",
      "  3%|█                                      | 4/153 [59:33<20:02:44, 484.33s/it]\u001b[A\n",
      "  3%|█▎                                     | 5/153 [59:34<12:44:38, 309.99s/it]\u001b[A\n",
      "  4%|█▌                                      | 6/153 [59:35<8:21:58, 204.89s/it]\u001b[A\n",
      "  5%|█▊                                      | 7/153 [59:36<5:36:15, 138.19s/it]\u001b[A\n",
      "  5%|██▏                                      | 8/153 [59:37<3:48:18, 94.47s/it]\u001b[A\n",
      "  6%|██▍                                      | 9/153 [59:38<2:36:29, 65.21s/it]\u001b[A\n",
      "  7%|██▌                                     | 10/153 [59:38<1:48:04, 45.34s/it]\u001b[A\n",
      "  7%|██▉                                     | 11/153 [59:39<1:15:05, 31.73s/it]\u001b[A\n",
      "  8%|███▎                                      | 12/153 [59:40<52:30, 22.34s/it]\u001b[A\n",
      " 33%|████████████▎                        | 1316/3948 [59:41<1:43:20,  2.36s/it]\u001b[A\n",
      "  9%|███▊                                      | 14/153 [59:42<26:12, 11.31s/it]\u001b[A\n",
      " 10%|████                                      | 15/153 [59:43<18:46,  8.16s/it]\u001b[A\n",
      " 10%|████▍                                     | 16/153 [59:44<13:37,  5.97s/it]\u001b[A\n",
      " 11%|████▋                                     | 17/153 [59:44<10:02,  4.43s/it]\u001b[A\n",
      " 12%|████▉                                     | 18/153 [59:45<07:33,  3.36s/it]\u001b[A\n",
      " 12%|█████▏                                    | 19/153 [59:46<05:49,  2.61s/it]\u001b[A\n",
      " 13%|█████▍                                    | 20/153 [59:47<04:37,  2.09s/it]\u001b[A\n",
      " 14%|█████▊                                    | 21/153 [59:48<03:46,  1.72s/it]\u001b[A\n",
      " 14%|██████                                    | 22/153 [59:49<03:11,  1.47s/it]\u001b[A\n",
      " 15%|██████▎                                   | 23/153 [59:50<02:46,  1.28s/it]\u001b[A\n",
      " 16%|██████▌                                   | 24/153 [59:51<02:29,  1.16s/it]\u001b[A\n",
      " 16%|██████▊                                   | 25/153 [59:51<02:17,  1.07s/it]\u001b[A\n",
      " 17%|███████▏                                  | 26/153 [59:52<02:08,  1.01s/it]\u001b[A\n",
      " 18%|███████▍                                  | 27/153 [59:53<02:01,  1.04it/s]\u001b[A\n",
      " 18%|███████▋                                  | 28/153 [59:54<01:57,  1.07it/s]\u001b[A\n",
      " 19%|███████▉                                  | 29/153 [59:55<01:53,  1.09it/s]\u001b[A\n",
      " 20%|████████▏                                 | 30/153 [59:56<01:50,  1.11it/s]\u001b[A\n",
      " 20%|████████▌                                 | 31/153 [59:57<01:48,  1.13it/s]\u001b[A\n",
      " 21%|████████▊                                 | 32/153 [59:57<01:46,  1.14it/s]\u001b[A\n",
      " 22%|█████████                                 | 33/153 [59:58<01:44,  1.15it/s]\u001b[A\n",
      " 22%|█████████▎                                | 34/153 [59:59<01:43,  1.15it/s]\u001b[A\n",
      " 23%|█████████▏                              | 35/153 [1:00:00<01:42,  1.16it/s]\u001b[A\n",
      " 24%|█████████▍                              | 36/153 [1:00:01<01:40,  1.16it/s]\u001b[A\n",
      " 24%|█████████▋                              | 37/153 [1:00:02<01:40,  1.16it/s]\u001b[A\n",
      " 25%|█████████▉                              | 38/153 [1:00:03<01:39,  1.16it/s]\u001b[A\n",
      " 25%|██████████▏                             | 39/153 [1:00:03<01:38,  1.16it/s]\u001b[A\n",
      " 26%|██████████▍                             | 40/153 [1:00:04<01:37,  1.16it/s]\u001b[A\n",
      " 27%|██████████▋                             | 41/153 [1:00:05<01:36,  1.16it/s]\u001b[A\n",
      " 27%|██████████▉                             | 42/153 [1:00:06<01:36,  1.15it/s]\u001b[A\n",
      " 28%|███████████▏                            | 43/153 [1:00:07<01:35,  1.15it/s]\u001b[A\n",
      " 29%|███████████▌                            | 44/153 [1:00:08<01:34,  1.15it/s]\u001b[A\n",
      " 29%|███████████▊                            | 45/153 [1:00:09<01:33,  1.15it/s]\u001b[A\n",
      " 30%|████████████                            | 46/153 [1:00:10<01:32,  1.16it/s]\u001b[A\n",
      " 31%|████████████▎                           | 47/153 [1:00:10<01:31,  1.16it/s]\u001b[A\n",
      " 31%|████████████▌                           | 48/153 [1:00:11<01:29,  1.17it/s]\u001b[A\n",
      " 32%|████████████▊                           | 49/153 [1:00:12<01:28,  1.18it/s]\u001b[A\n",
      " 33%|█████████████                           | 50/153 [1:00:13<01:27,  1.18it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** exception *****  tensor([5, 5, 5,  ..., 5, 5, 5])\n",
      "the best f1 is now: 0.18261064351375839\n",
      "{'RED': {'precision': 0.8181818181818182, 'recall': 0.3, 'f1': 0.43902439024390244, 'number': 60}, 'RG0': {'precision': 0.8, 'recall': 0.2222222222222222, 'f1': 0.3478260869565218, 'number': 18}, 'RG1': {'precision': 0.5454545454545454, 'recall': 0.10714285714285714, 'f1': 0.1791044776119403, 'number': 56}, 'RG2': {'precision': 0.5, 'recall': 0.125, 'f1': 0.2, 'number': 8}, 'UPPORT': {'precision': 1.0, 'recall': 0.25, 'f1': 0.4, 'number': 20}, 'one': {'precision': 0.3431372549019608, 'recall': 0.19230769230769232, 'f1': 0.24647887323943662, 'number': 182}, 'overall_precision': 0.46938775510204084, 'overall_recall': 0.2005813953488372, 'overall_f1': 0.28105906313645623, 'overall_accuracy': 0.915864166244298}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|████████████████████████▋            | 2632/3948 [1:58:42<44:33,  2.03s/it]\n",
      " 33%|███████████▋                       | 51/153 [1:58:43<29:50:58, 1053.51s/it]\u001b[A\n",
      " 34%|████████████▏                       | 52/153 [1:58:43<20:41:48, 737.71s/it]\u001b[A\n",
      " 35%|████████████▍                       | 53/153 [1:58:44<14:21:05, 516.65s/it]\u001b[A\n",
      " 35%|█████████████                        | 54/153 [1:58:45<9:57:09, 361.91s/it]\u001b[A\n",
      " 36%|█████████████▎                       | 55/153 [1:58:46<6:54:11, 253.59s/it]\u001b[A\n",
      " 37%|█████████████▌                       | 56/153 [1:58:47<4:47:23, 177.76s/it]\u001b[A\n",
      " 37%|█████████████▊                       | 57/153 [1:58:48<3:19:30, 124.69s/it]\u001b[A\n",
      " 38%|██████████████▍                       | 58/153 [1:58:49<2:18:36, 87.54s/it]\u001b[A\n",
      " 39%|██████████████▋                       | 59/153 [1:58:49<1:36:23, 61.53s/it]\u001b[A\n",
      " 39%|██████████████▉                       | 60/153 [1:58:50<1:07:09, 43.33s/it]\u001b[A\n",
      " 40%|███████████████▉                        | 61/153 [1:58:51<46:53, 30.58s/it]\u001b[A\n",
      " 41%|████████████████▏                       | 62/153 [1:58:52<32:51, 21.67s/it]\u001b[A\n",
      " 41%|████████████████▍                       | 63/153 [1:58:53<23:07, 15.42s/it]\u001b[A\n",
      " 42%|████████████████▋                       | 64/153 [1:58:54<16:23, 11.05s/it]\u001b[A\n",
      " 42%|████████████████▉                       | 65/153 [1:58:55<11:43,  8.00s/it]\u001b[A\n",
      " 43%|█████████████████▎                      | 66/153 [1:58:55<08:28,  5.85s/it]\u001b[A\n",
      " 44%|█████████████████▌                      | 67/153 [1:58:56<06:13,  4.35s/it]\u001b[A\n",
      " 44%|█████████████████▊                      | 68/153 [1:58:57<04:40,  3.30s/it]\u001b[A\n",
      " 45%|██████████████████                      | 69/153 [1:58:58<03:34,  2.56s/it]\u001b[A\n",
      " 46%|██████████████████▎                     | 70/153 [1:58:59<02:50,  2.05s/it]\u001b[A\n",
      " 46%|██████████████████▌                     | 71/153 [1:59:00<02:18,  1.69s/it]\u001b[A\n",
      " 47%|██████████████████▊                     | 72/153 [1:59:00<01:56,  1.44s/it]\u001b[A\n",
      " 48%|███████████████████                     | 73/153 [1:59:01<01:40,  1.26s/it]\u001b[A\n",
      " 48%|███████████████████▎                    | 74/153 [1:59:02<01:29,  1.14s/it]\u001b[A\n",
      " 49%|███████████████████▌                    | 75/153 [1:59:03<01:22,  1.05s/it]\u001b[A\n",
      " 50%|███████████████████▊                    | 76/153 [1:59:04<01:16,  1.01it/s]\u001b[A\n",
      " 50%|████████████████████▏                   | 77/153 [1:59:05<01:11,  1.06it/s]\u001b[A\n",
      " 51%|████████████████████▍                   | 78/153 [1:59:06<01:08,  1.09it/s]\u001b[A\n",
      " 52%|████████████████████▋                   | 79/153 [1:59:06<01:05,  1.12it/s]\u001b[A/Users/jiajingchen/miniforge3/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 52%|████████████████████▉                   | 80/153 [1:59:07<01:03,  1.14it/s]\u001b[A\n",
      " 53%|█████████████████████▏                  | 81/153 [1:59:08<01:02,  1.15it/s]\u001b[A\n",
      " 54%|█████████████████████▍                  | 82/153 [1:59:09<01:01,  1.16it/s]\u001b[A\n",
      " 54%|█████████████████████▋                  | 83/153 [1:59:10<00:59,  1.17it/s]\u001b[A\n",
      " 55%|█████████████████████▉                  | 84/153 [1:59:11<00:58,  1.18it/s]\u001b[A\n",
      " 56%|██████████████████████▏                 | 85/153 [1:59:11<00:57,  1.18it/s]\u001b[A\n",
      " 56%|██████████████████████▍                 | 86/153 [1:59:12<00:56,  1.19it/s]\u001b[A\n",
      " 57%|██████████████████████▋                 | 87/153 [1:59:13<00:55,  1.18it/s]\u001b[A\n",
      " 58%|███████████████████████                 | 88/153 [1:59:14<00:54,  1.18it/s]\u001b[A\n",
      " 58%|███████████████████████▎                | 89/153 [1:59:15<00:53,  1.19it/s]\u001b[A\n",
      " 59%|███████████████████████▌                | 90/153 [1:59:16<00:53,  1.18it/s]\u001b[A\n",
      " 59%|███████████████████████▊                | 91/153 [1:59:16<00:52,  1.19it/s]\u001b[A\n",
      " 60%|████████████████████████                | 92/153 [1:59:17<00:51,  1.18it/s]\u001b[A\n",
      " 61%|████████████████████████▎               | 93/153 [1:59:18<00:50,  1.18it/s]\u001b[A\n",
      " 61%|████████████████████████▌               | 94/153 [1:59:19<00:49,  1.18it/s]\u001b[A\n",
      " 62%|████████████████████████▊               | 95/153 [1:59:20<00:48,  1.18it/s]\u001b[A\n",
      " 63%|█████████████████████████               | 96/153 [1:59:21<00:47,  1.19it/s]\u001b[A\n",
      " 63%|█████████████████████████▎              | 97/153 [1:59:21<00:46,  1.20it/s]\u001b[A\n",
      " 64%|█████████████████████████▌              | 98/153 [1:59:22<00:45,  1.21it/s]\u001b[A\n",
      " 65%|█████████████████████████▉              | 99/153 [1:59:23<00:44,  1.21it/s]\u001b[A\n",
      " 65%|█████████████████████████▍             | 100/153 [1:59:24<00:43,  1.21it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** exception *****  tensor([5, 5, 5,  ..., 5, 5, 5])\n",
      "the best f1 is now: 0.19143647578504988\n",
      "{'RED': {'precision': 0.8636363636363636, 'recall': 0.31666666666666665, 'f1': 0.46341463414634154, 'number': 60}, 'RG0': {'precision': 0.8333333333333334, 'recall': 0.2777777777777778, 'f1': 0.4166666666666667, 'number': 18}, 'RG1': {'precision': 0.47619047619047616, 'recall': 0.17857142857142858, 'f1': 0.2597402597402597, 'number': 56}, 'RG2': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 8}, 'UPPORT': {'precision': 1.0, 'recall': 0.1, 'f1': 0.18181818181818182, 'number': 20}, 'one': {'precision': 0.39622641509433965, 'recall': 0.23076923076923078, 'f1': 0.2916666666666667, 'number': 182}, 'overall_precision': 0.4968152866242038, 'overall_recall': 0.22674418604651161, 'overall_f1': 0.31137724550898205, 'overall_accuracy': 0.9199189052204765}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 3948/3948 [2:58:28<00:00,  1.99s/it]\n",
      " 66%|██████████████████████▍           | 101/153 [2:58:29<15:22:08, 1064.02s/it]\u001b[A\n",
      " 67%|███████████████████████▎           | 102/153 [2:58:30<10:33:18, 745.06s/it]\u001b[A\n",
      " 67%|████████████████████████▏           | 103/153 [2:58:30<7:14:49, 521.79s/it]\u001b[A\n",
      " 68%|████████████████████████▍           | 104/153 [2:58:31<4:58:29, 365.51s/it]\u001b[A\n",
      " 69%|████████████████████████▋           | 105/153 [2:58:32<3:24:52, 256.10s/it]\u001b[A\n",
      " 69%|████████████████████████▉           | 106/153 [2:58:33<2:20:37, 179.52s/it]\u001b[A\n",
      " 70%|█████████████████████████▏          | 107/153 [2:58:34<1:36:32, 125.92s/it]\u001b[A\n",
      " 71%|██████████████████████████           | 108/153 [2:58:35<1:06:17, 88.40s/it]\u001b[A\n",
      " 71%|███████████████████████████▊           | 109/153 [2:58:35<45:33, 62.13s/it]\u001b[A\n",
      " 72%|████████████████████████████           | 110/153 [2:58:36<31:20, 43.74s/it]\u001b[A\n",
      " 73%|████████████████████████████▎          | 111/153 [2:58:37<21:36, 30.87s/it]\u001b[A\n",
      " 73%|████████████████████████████▌          | 112/153 [2:58:38<14:56, 21.86s/it]\u001b[A\n",
      " 74%|████████████████████████████▊          | 113/153 [2:58:39<10:22, 15.55s/it]\u001b[A\n",
      " 75%|█████████████████████████████          | 114/153 [2:58:40<07:14, 11.14s/it]\u001b[A\n",
      " 75%|█████████████████████████████▎         | 115/153 [2:58:40<05:05,  8.05s/it]\u001b[A\n",
      " 76%|█████████████████████████████▌         | 116/153 [2:58:41<03:37,  5.89s/it]\u001b[A\n",
      " 76%|█████████████████████████████▊         | 117/153 [2:58:42<02:37,  4.37s/it]\u001b[A\n",
      " 77%|██████████████████████████████         | 118/153 [2:58:43<01:55,  3.31s/it]\u001b[A\n",
      " 78%|██████████████████████████████▎        | 119/153 [2:58:44<01:27,  2.57s/it]\u001b[A\n",
      " 78%|██████████████████████████████▌        | 120/153 [2:58:45<01:07,  2.04s/it]\u001b[A\n",
      " 79%|██████████████████████████████▊        | 121/153 [2:58:45<00:53,  1.68s/it]\u001b[A\n",
      " 80%|███████████████████████████████        | 122/153 [2:58:46<00:44,  1.42s/it]\u001b[A\n",
      " 80%|███████████████████████████████▎       | 123/153 [2:58:47<00:37,  1.25s/it]\u001b[A\n",
      " 81%|███████████████████████████████▌       | 124/153 [2:58:48<00:32,  1.12s/it]\u001b[A\n",
      " 82%|███████████████████████████████▊       | 125/153 [2:58:49<00:29,  1.04s/it]\u001b[A\n",
      " 82%|████████████████████████████████       | 126/153 [2:58:50<00:26,  1.02it/s]\u001b[A\n",
      " 83%|████████████████████████████████▎      | 127/153 [2:58:50<00:24,  1.06it/s]\u001b[A\n",
      " 84%|████████████████████████████████▋      | 128/153 [2:58:51<00:22,  1.10it/s]\u001b[A\n",
      " 84%|████████████████████████████████▉      | 129/153 [2:58:52<00:21,  1.13it/s]\u001b[A\n",
      " 85%|█████████████████████████████████▏     | 130/153 [2:58:53<00:19,  1.15it/s]\u001b[A\n",
      " 86%|█████████████████████████████████▍     | 131/153 [2:58:54<00:18,  1.17it/s]\u001b[A\n",
      " 86%|█████████████████████████████████▋     | 132/153 [2:58:55<00:17,  1.17it/s]\u001b[A\n",
      " 87%|█████████████████████████████████▉     | 133/153 [2:58:55<00:16,  1.19it/s]\u001b[A\n",
      " 88%|██████████████████████████████████▏    | 134/153 [2:58:56<00:16,  1.18it/s]\u001b[A\n",
      " 88%|██████████████████████████████████▍    | 135/153 [2:58:57<00:15,  1.18it/s]\u001b[A\n",
      " 89%|██████████████████████████████████▋    | 136/153 [2:58:58<00:14,  1.18it/s]\u001b[A\n",
      " 90%|██████████████████████████████████▉    | 137/153 [2:58:59<00:13,  1.19it/s]\u001b[A\n",
      " 90%|███████████████████████████████████▏   | 138/153 [2:59:00<00:12,  1.19it/s]\u001b[A\n",
      " 91%|███████████████████████████████████▍   | 139/153 [2:59:01<00:11,  1.18it/s]\u001b[A\n",
      " 92%|███████████████████████████████████▋   | 140/153 [2:59:01<00:10,  1.18it/s]\u001b[A\n",
      " 92%|███████████████████████████████████▉   | 141/153 [2:59:02<00:10,  1.19it/s]\u001b[A\n",
      " 93%|████████████████████████████████████▏  | 142/153 [2:59:03<00:09,  1.19it/s]\u001b[A\n",
      " 93%|████████████████████████████████████▍  | 143/153 [2:59:04<00:08,  1.20it/s]\u001b[A\n",
      " 94%|████████████████████████████████████▋  | 144/153 [2:59:05<00:07,  1.19it/s]\u001b[A\n",
      " 95%|████████████████████████████████████▉  | 145/153 [2:59:06<00:06,  1.19it/s]\u001b[A\n",
      " 95%|█████████████████████████████████████▏ | 146/153 [2:59:06<00:05,  1.19it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████▍ | 147/153 [2:59:07<00:05,  1.20it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████▋ | 148/153 [2:59:08<00:04,  1.21it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████▉ | 149/153 [2:59:09<00:03,  1.22it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████▏| 150/153 [2:59:10<00:02,  1.22it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** exception *****  tensor([5, 5, 5,  ..., 5, 5, 5])\n",
      "the best f1 is now: 0.2158768160279965\n",
      "{'RED': {'precision': 0.8518518518518519, 'recall': 0.38333333333333336, 'f1': 0.5287356321839081, 'number': 60}, 'RG0': {'precision': 0.7142857142857143, 'recall': 0.2777777777777778, 'f1': 0.4, 'number': 18}, 'RG1': {'precision': 0.5, 'recall': 0.21428571428571427, 'f1': 0.3, 'number': 56}, 'RG2': {'precision': 0.3333333333333333, 'recall': 0.125, 'f1': 0.18181818181818182, 'number': 8}, 'UPPORT': {'precision': 1.0, 'recall': 0.2, 'f1': 0.33333333333333337, 'number': 20}, 'one': {'precision': 0.4396551724137931, 'recall': 0.2802197802197802, 'f1': 0.3422818791946309, 'number': 182}, 'overall_precision': 0.5303867403314917, 'overall_recall': 0.27906976744186046, 'overall_f1': 0.36571428571428566, 'overall_accuracy': 0.922453117080588}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW,get_scheduler\n",
    "from datasets import load_metric\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "num_epochs = 3\n",
    "model_cc = CustomModel(checkpoint=model_checkpoint,num_labels=len(label_names))#.cuda()\n",
    "optimizer = AdamW(model_cc.parameters(), lr=2e-5)\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "progress_bar_train = tqdm(range(num_training_steps),miniters=2)\n",
    "progress_bar_eval = tqdm(range(num_epochs * len(eval_dataloader)),miniters=2)\n",
    "f1_best = 0\n",
    "resume_flag = False\n",
    "best_net = None\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    if resume_flag:\n",
    "        model_cc.load_state_dict(torch.load(\"sstcls_best.dat\"))\n",
    "    model_cc.train()\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v for k, v in batch.items()}#.cuda()\n",
    "        outputs = model_cc(batch['input_ids'], token_type_ids=batch['token_type_ids'], attention_mask=batch['attention_mask'],labels=batch['labels'], POS=batch['POSL'], BIO=batch['BIOL'])\n",
    "      \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar_train.update(1)\n",
    "    model_cc.eval()\n",
    "    f1_now = []\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v for k, v in batch.items()}#.cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs = model_cc(batch['input_ids'], token_type_ids=batch['token_type_ids'], attention_mask=batch['attention_mask'],labels=batch['labels'], POS=batch['POSL'], BIO=batch['BIOL'])\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        true_labels = [[label_names[l] for l in label if l != -100] for label in batch[\"labels\"]]\n",
    "        \n",
    "        try:\n",
    "            reshaped_predictions = torch.reshape(predictions, (BatchSize,-1))\n",
    "            true_predictions = [\n",
    "                [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "                for prediction, label in zip(reshaped_predictions, batch[\"labels\"])\n",
    "            ]\n",
    "\n",
    "            all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "            metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "            progress_bar_eval.update(1)\n",
    "            f1_now.append(all_metrics[\"overall_f1\"])\n",
    "        except:\n",
    "            print(\"***** exception ***** \",predictions)\n",
    "            f1_now.append(0)\n",
    "        \n",
    "    if np.mean(f1_now) > f1_best or not best_net:\n",
    "        torch.save(model_cc.state_dict(), 'sstcls_best.dat')\n",
    "        f1_best = np.mean(f1_now)\n",
    "        print(\"the best f1 is now: \"+ str(np.mean(f1_now)))\n",
    "        best_net = model_cc\n",
    "        \n",
    "\n",
    "    print(metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00dea989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, dataloader, gpu):\n",
    "    net.eval()\n",
    "\n",
    "    f1_now = []\n",
    "    precision_now = []\n",
    "    recall_now = []\n",
    "    accuracy_now = []\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v for k, v in batch.items()}#.cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs = model_cc(batch['input_ids'], token_type_ids=batch['token_type_ids'], attention_mask=batch['attention_mask'], labels=batch['labels'], POS=batch['POSL'], BIO=batch['BIOL'])\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        true_labels = [[label_names[l] for l in label if l != -100] for label in batch[\"labels\"]]\n",
    "        \n",
    "        try:\n",
    "            reshaped_predictions = torch.reshape(predictions, (BatchSize,-1))\n",
    "            true_predictions = [\n",
    "                [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "                for prediction, label in zip(reshaped_predictions, batch[\"labels\"])\n",
    "            ]\n",
    "\n",
    "            all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "            metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "            progress_bar_eval.update(1)\n",
    "            f1_now.append(all_metrics[\"overall_f1\"])\n",
    "            precision_now.append(all_metrics[\"overall_precision\"])\n",
    "            recall_now.append(all_metrics[\"overall_recall\"])\n",
    "            accuracy_now.append(all_metrics[\"overall_accuracy\"])\n",
    "        \n",
    "        except:\n",
    "            print(\"***** exception ***** \",predictions)\n",
    "            f1_now.append(0)\n",
    "            precision_now.append(0)\n",
    "            recall_now.append(0)\n",
    "            accuracy_now.append(0)\n",
    "\n",
    "\n",
    "    return np.mean(f1_now), np.mean(precision_now), np.mean(recall_now), np.mean(accuracy_now),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aca124a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 99%|██████████████████████████████████████▍| 151/153 [2:59:11<00:02,  1.06s/it]\u001b[A\n",
      " 99%|██████████████████████████████████████▋| 152/153 [2:59:12<00:00,  1.00it/s]\u001b[A\n",
      "100%|███████████████████████████████████████| 153/153 [2:59:13<00:00,  1.06it/s]\u001b[A\n",
      "154it [2:59:14,  1.10it/s]                                                      \u001b[A\n",
      "155it [2:59:15,  1.13it/s]\u001b[A\n",
      "156it [2:59:15,  1.15it/s]\u001b[A\n",
      "157it [2:59:16,  1.17it/s]\u001b[A\n",
      "158it [2:59:17,  1.19it/s]\u001b[A\n",
      "159it [2:59:18,  1.20it/s]\u001b[A\n",
      "160it [2:59:19,  1.21it/s]\u001b[A\n",
      "161it [2:59:20,  1.21it/s]\u001b[A\n",
      "162it [2:59:20,  1.22it/s]\u001b[A\n",
      "163it [2:59:21,  1.22it/s]\u001b[A\n",
      "164it [2:59:22,  1.22it/s]\u001b[A\n",
      "165it [2:59:23,  1.22it/s]\u001b[A\n",
      "166it [2:59:24,  1.22it/s]\u001b[A\n",
      "167it [2:59:24,  1.22it/s]\u001b[A\n",
      "168it [2:59:25,  1.22it/s]\u001b[A\n",
      "169it [2:59:26,  1.22it/s]\u001b[A\n",
      "170it [2:59:27,  1.22it/s]\u001b[A\n",
      "171it [2:59:28,  1.22it/s]\u001b[A\n",
      "172it [2:59:29,  1.22it/s]\u001b[A\n",
      "173it [2:59:29,  1.22it/s]\u001b[A\n",
      "174it [2:59:30,  1.22it/s]\u001b[A\n",
      "175it [2:59:31,  1.22it/s]\u001b[A\n",
      "176it [2:59:32,  1.22it/s]\u001b[A\n",
      "177it [2:59:33,  1.22it/s]\u001b[A\n",
      "178it [2:59:33,  1.22it/s]\u001b[A\n",
      "179it [2:59:34,  1.22it/s]\u001b[A\n",
      "180it [2:59:35,  1.22it/s]\u001b[A\n",
      "181it [2:59:36,  1.22it/s]\u001b[A\n",
      "182it [2:59:37,  1.22it/s]\u001b[A\n",
      "183it [2:59:38,  1.21it/s]\u001b[A\n",
      "184it [2:59:38,  1.21it/s]\u001b[A\n",
      "185it [2:59:39,  1.21it/s]\u001b[A\n",
      "186it [2:59:40,  1.21it/s]\u001b[A\n",
      "187it [2:59:41,  1.21it/s]\u001b[A\n",
      "188it [2:59:42,  1.20it/s]\u001b[A\n",
      "189it [2:59:43,  1.21it/s]\u001b[A\n",
      "190it [2:59:43,  1.21it/s]\u001b[A\n",
      "191it [2:59:44,  1.22it/s]\u001b[A\n",
      "192it [2:59:45,  1.22it/s]\u001b[A\n",
      "193it [2:59:46,  1.22it/s]\u001b[A\n",
      "194it [2:59:47,  1.22it/s]\u001b[A\n",
      "195it [2:59:47,  1.22it/s]\u001b[A\n",
      "196it [2:59:48,  1.22it/s]\u001b[A\n",
      "197it [2:59:49,  1.23it/s]\u001b[A\n",
      "198it [2:59:50,  1.24it/s]\u001b[A\n",
      "199it [2:59:51,  1.23it/s]\u001b[A\n",
      "200it [2:59:51,  1.24it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** exception *****  tensor([5, 5, 5,  ..., 5, 5, 5])\n",
      "test_f1_score:::0.2158768160279965\n",
      "test_precision_score:::0.3337354846008516\n",
      "test_recall_score:::0.16008594669369358\n",
      "test_accuracy_score:::0.9009202937390933\n"
     ]
    }
   ],
   "source": [
    "preds = predict(best_net, test_dataloader, 0)\n",
    "print(\"test_f1_score:::\"+str(preds[0]))\n",
    "print(\"test_precision_score:::\"+str(preds[1]))\n",
    "print(\"test_recall_score:::\"+str(preds[2]))\n",
    "print(\"test_accuracy_score:::\"+str(preds[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da1c4de-27f1-4e49-be87-655003d4ad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Both \n",
    "\"\"\"\n",
    "test_f1_score:::0.2158768160279965\n",
    "test_precision_score:::0.3337354846008516\n",
    "test_recall_score:::0.16008594669369358\n",
    "test_accuracy_score:::0.9009202937390933\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
