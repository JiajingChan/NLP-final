{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "678a7b49-66d2-47ee-8c00-d29cacb92784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. build our own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2116af7-acbd-4fdf-ab5e-80ef11677cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'POS', 'BIO', 'label', 'id'],\n",
       "        num_rows: 2174\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'POS', 'BIO', 'label', 'id'],\n",
       "        num_rows: 83\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'POS', 'BIO', 'label', 'id'],\n",
       "        num_rows: 150\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# map POS to POS_id\n",
    "cnt = 0\n",
    "POS_id = {}\n",
    "POS_ls = ['NN', 'IN', 'NNP', 'DT', 'NNS', 'JJ', 'COMMA', 'CD', '.', 'VBD', 'RB','VB', 'CC', 'VBN', 'VBZ', \n",
    "          'VBG', 'TO', 'PRP', 'VBP', 'POS', 'PRP$','MD', '$', '``', \"''\", 'WDT', ':', 'JJR', 'RP', 'RBR', \n",
    "          'WP', 'NNPS','JJS', ')', '(', 'EX', 'RBS', 'WRB', '-', 'UH', 'WP$', 'PDT', '/', '#', 'LS', 'SYM', 'FW', 'AUX']\n",
    "for pos in POS_ls:\n",
    "    POS_id[pos] = cnt\n",
    "    cnt += 1\n",
    "\n",
    "# map BIO to BIO_id\n",
    "cnt = 0\n",
    "BIO_id = {}\n",
    "BIO_ls = ['O', 'B-NP', 'I-NP', 'B-PP', 'B-ADVP', 'B-ADJP', 'B-SBAR', 'B-CONJP',\n",
    "       'I-ADJP', 'I-PP', 'I-ADVP', 'I-CONJP', 'B-INTJ', 'I-SBAR', 'B-LST',\n",
    "       'B-VP', 'B-PRT', 'I-INTJ', 'I-VP']\n",
    "for bio in BIO_ls:\n",
    "    BIO_id[bio] = cnt\n",
    "    cnt += 1\n",
    "\n",
    "# map label to BIO_id\n",
    "Label_id = {\"ARG0\":0,\"ARG1\":1,\"ARG2\":2,\"PRED\":3,\"SUPPORT\":4}\n",
    "def mapLabel(label):\n",
    "    return Label_id[label] if label in Label_id else 5\n",
    "\n",
    "# build datasets\n",
    "def condense_df(file):\n",
    "    df = pd.DataFrame()\n",
    "    with open(file, 'r') as file:\n",
    "        ls = [i.split('\\t') for i in file.read().split('\\n')]\n",
    "        df = pd.DataFrame(ls)\n",
    "\n",
    "    df['id'] = df.index\n",
    "    df[0].replace('', np.nan, inplace=True)\n",
    "    df.dropna(axis=0, subset = [0], inplace = True)\n",
    "    df['BIO'] = df[2].map(BIO_id)\n",
    "    df['POS'] = df[1].map(POS_id)\n",
    "    df['label'] = df[5].map(mapLabel)\n",
    "    df['id'] = df[4].map(int)\n",
    "    df.drop(columns = [1, 2, 3, 4, 5, 6], inplace = True)\n",
    "    condense = df.groupby('id').apply(lambda x: [list(x[0]),list(x['POS']), list(x['BIO']), list(x['label'])]).apply(pd.Series)\n",
    "    condense.columns =['tokens','POS','BIO','label']\n",
    "    return condense\n",
    "\n",
    "\n",
    "train = Dataset.from_pandas(condense_df(\"Partitive-Files/%_nombank.clean.train\"))\n",
    "eval_ = Dataset.from_pandas(condense_df(\"Partitive-Files/%_nombank.clean.dev\"))\n",
    "test = Dataset.from_pandas(condense_df(\"Partitive-Files/%_nombank.clean.test\"))\n",
    "datasets = DatasetDict({\"train\": train, \"validation\":eval_, \"test\":test})\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad2a9519-741f-4eb3-a074-2af5e116fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9322db92-e1ac-4c82-9f7c-86d7f9b75943",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c72fd67667e40baa4f54d851448a9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "678c1a03270c40e3a75a92b0acef5369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e2e20936e943afb441fdae7904e483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if not word_id:\n",
    "            new_labels.append(6)\n",
    "        else:\n",
    "            if word_id != current_word:# Start of a new word!\n",
    "                current_word = word_id       \n",
    "            label = labels[word_id]\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    all_labels = examples[\"label\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns = datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa27677f-cf86-4752-b389-505d32e45f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. train while evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d03d8e7-778d-4d0f-b6d2-d5c940a71b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "small_train = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(100))\n",
    "small_eval = tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(30))\n",
    "small_test = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(30))\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=1, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=1, collate_fn=data_collator\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"test\"], batch_size=1, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8c02d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\"ARG0\", \"ARG1\", \"ARG2\", \"PRED\", \"SUPPORT\", \"None\", \"-100\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb96307d-f409-4bf5-9cb2-63557118360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_metrics(eval_preds):\n",
    "#     logits, labels = eval_preds\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "#     # Remove ignored index (special tokens) and convert to labels\n",
    "#     true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "#     true_predictions = [\n",
    "#         [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "#         for prediction, label in zip(predictions, labels)\n",
    "#     ]\n",
    "#     all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "#     return {\n",
    "#         \"precision\": all_metrics[\"overall_precision\"],\n",
    "#         \"recall\": all_metrics[\"overall_recall\"],\n",
    "#         \"f1\": all_metrics[\"overall_f1\"],\n",
    "#         \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "#     }\n",
    "# id2label = {str(i): label for i, label in enumerate(label_names)}\n",
    "# label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\n",
    "#     model_checkpoint,\n",
    "#     id2label=id2label,\n",
    "#     label2id=label2id,\n",
    "#     output_attentions=True,\n",
    "#     output_hidden_states=True\n",
    "# )\n",
    "\n",
    "# args = TrainingArguments(\n",
    "#     output_dir = \"bert-finetuned\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model = model,\n",
    "#     args = args,\n",
    "#     train_dataset = tokenized_datasets[\"train\"],#small_train,#\n",
    "#     eval_dataset = tokenized_datasets[\"validation\"],#small_eval,#\n",
    "#     data_collator = data_collator,\n",
    "#     compute_metrics = compute_metrics,\n",
    "#     tokenizer = tokenizer,\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61d8796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "import torch.nn as nn\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self,checkpoint,num_labels): \n",
    "        super(CustomModel,self).__init__() \n",
    "        self.num_labels = num_labels \n",
    "\n",
    "        #Load Model with given checkpoint and extract its body\n",
    "        self.model = AutoModel.from_pretrained(checkpoint,config=AutoConfig.from_pretrained(checkpoint, output_attentions=True,output_hidden_states=True))\n",
    "        self.dropout = nn.Dropout(0.1) \n",
    "        self.classifier = nn.Linear(768,num_labels) # load and initialize weights\n",
    "\n",
    "    def forward(self, input_ids=None, token_type_ids=None, attention_mask=None,labels=None):\n",
    "        #Extract outputs from the body\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         print(\"input_ids:::\"+str(input_ids.shape))\n",
    "#         print(\"labels:::\"+str(labels.shape))\n",
    "        #Add custom layers\n",
    "        sequence_output = self.dropout(outputs[0]) #outputs[0]=last hidden state\n",
    "#         print(\"sequence_output:::\"+str(sequence_output.shape))\n",
    "\n",
    "        logits = self.classifier(sequence_output[:,:,:].view(-1, 768)) # calculate losses\n",
    "        self.logits = logits\n",
    "#         print(\"logits::\"+str(logits.shape))\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states,attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a0c4d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/chao1804/.conda/envs/NLP/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|                                                 | 0/6522 [00:00<?, ?it/s]\n",
      "  0%|                                                  | 0/249 [00:00<?, ?it/s]\u001b[AYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 33%|████████████▋                         | 2172/6522 [01:44<03:27, 20.92it/s]/home/chao1804/.conda/envs/NLP/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: None seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/chao1804/.conda/envs/NLP/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRED seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/chao1804/.conda/envs/NLP/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SUPPORT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/chao1804/.conda/envs/NLP/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ARG1 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "\n",
      "  0%|▏                                      | 1/249 [01:44<7:12:37, 104.67s/it]\u001b[A/home/chao1804/.conda/envs/NLP/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "  3%|█▏                                        | 7/249 [01:44<44:10, 10.95s/it]\u001b[A\n",
      "  6%|██▎                                      | 14/249 [01:44<17:14,  4.40s/it]\u001b[A\n",
      "  8%|███▍                                     | 21/249 [01:44<09:02,  2.38s/it]\u001b[A\n",
      " 12%|████▊                                    | 29/249 [01:45<04:59,  1.36s/it]\u001b[A\n",
      " 14%|█████▉                                   | 36/249 [01:45<03:10,  1.12it/s]\u001b[A\n",
      " 17%|███████                                  | 43/249 [01:45<02:03,  1.67it/s]\u001b[A\n",
      " 20%|████████▍                                | 51/249 [01:45<01:17,  2.54it/s]\u001b[A/home/chao1804/.conda/envs/NLP/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 23%|█████████▌                               | 58/249 [01:45<00:53,  3.60it/s]\u001b[A\n",
      " 27%|██████████▊                              | 66/249 [01:45<00:34,  5.27it/s]\u001b[A\n",
      " 30%|████████████▏                            | 74/249 [01:45<00:23,  7.53it/s]\u001b[A\n",
      " 33%|█████████████▌                           | 82/249 [01:45<00:16, 10.41it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best f1 is now: 0.8013660748738027\n",
      "{'RED': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1}, 'one': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2}, 'overall_precision': 1.0, 'overall_recall': 1.0, 'overall_f1': 1.0, 'overall_accuracy': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|█████████████▋                        | 2340/6522 [01:56<03:26, 20.25it/s]\n",
      " 67%|█████████████████████████▎            | 4347/6522 [03:33<01:44, 20.91it/s]\u001b[A\n",
      " 34%|█████████████▊                           | 84/249 [03:33<15:14,  5.54s/it]\u001b[A\n",
      " 37%|███████████████▏                         | 92/249 [03:33<09:11,  3.51s/it]\u001b[A\n",
      " 40%|████████████████                        | 100/249 [03:33<05:44,  2.31s/it]\u001b[A\n",
      " 43%|█████████████████▎                      | 108/249 [03:33<03:39,  1.56s/it]\u001b[A\n",
      " 47%|██████████████████▋                     | 116/249 [03:34<02:21,  1.06s/it]\u001b[A\n",
      " 50%|███████████████████▉                    | 124/249 [03:34<01:31,  1.36it/s]\u001b[A\n",
      " 53%|█████████████████████▏                  | 132/249 [03:34<00:59,  1.96it/s]\u001b[A\n",
      " 56%|██████████████████████▍                 | 140/249 [03:34<00:39,  2.79it/s]\u001b[A\n",
      " 59%|███████████████████████▊                | 148/249 [03:34<00:25,  3.95it/s]\u001b[A\n",
      " 63%|█████████████████████████               | 156/249 [03:34<00:16,  5.54it/s]\u001b[A\n",
      " 66%|██████████████████████████▎             | 164/249 [03:34<00:11,  7.68it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RED': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1}, 'one': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2}, 'overall_precision': 1.0, 'overall_recall': 1.0, 'overall_f1': 1.0, 'overall_accuracy': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████▋           | 4584/6522 [03:46<01:32, 20.87it/s]\n",
      "100%|██████████████████████████████████████| 6522/6522 [05:20<00:00, 20.77it/s]\u001b[A\n",
      " 67%|██████████████████████████▊             | 167/249 [05:20<06:51,  5.01s/it]\u001b[A\n",
      " 70%|████████████████████████████            | 175/249 [05:20<04:02,  3.28s/it]\u001b[A/home/chao1804/.conda/envs/NLP/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: -100 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "\n",
      " 73%|█████████████████████████████▍          | 183/249 [05:20<02:25,  2.20s/it]\u001b[A\n",
      " 77%|██████████████████████████████▋         | 191/249 [05:20<01:26,  1.50s/it]\u001b[A\n",
      " 80%|███████████████████████████████▉        | 199/249 [05:20<00:51,  1.03s/it]\u001b[A\n",
      " 83%|█████████████████████████████████▎      | 207/249 [05:20<00:30,  1.40it/s]\u001b[A\n",
      " 86%|██████████████████████████████████▌     | 215/249 [05:20<00:16,  2.00it/s]\u001b[A\n",
      " 90%|███████████████████████████████████▊    | 223/249 [05:21<00:09,  2.84it/s]\u001b[A\n",
      " 93%|█████████████████████████████████████   | 231/249 [05:21<00:04,  4.02it/s]\u001b[A\n",
      " 96%|██████████████████████████████████████▍ | 239/249 [05:21<00:01,  5.63it/s]\u001b[A\n",
      " 99%|███████████████████████████████████████▋| 247/249 [05:21<00:00,  7.80it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RED': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1}, 'one': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2}, 'overall_precision': 1.0, 'overall_recall': 1.0, 'overall_f1': 1.0, 'overall_accuracy': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW,get_scheduler\n",
    "from datasets import load_metric\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "num_epochs = 3\n",
    "model_cc = CustomModel(checkpoint=model_checkpoint,num_labels=len(label_names)).cuda()\n",
    "optimizer = AdamW(model_cc.parameters(), lr=2e-5)\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "progress_bar_train = tqdm(range(num_training_steps))\n",
    "progress_bar_eval = tqdm(range(num_epochs * len(eval_dataloader)))\n",
    "f1_best = 0\n",
    "resume_flag = True\n",
    "best_net = None\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    if resume_flag:\n",
    "        model_cc.load_state_dict(torch.load(\"sstcls_best.dat\"))\n",
    "    model_cc.train()\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.cuda() for k, v in batch.items()}\n",
    "        outputs = model_cc(batch['input_ids'], token_type_ids=batch['token_type_ids'], attention_mask=batch['attention_mask'], labels=batch['labels'])\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar_train.update(1)\n",
    "    model_cc.eval()\n",
    "    f1_now = []\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.cuda() for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model_cc(batch['input_ids'], token_type_ids=batch['token_type_ids'], attention_mask=batch['attention_mask'], labels=batch['labels'])\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        true_labels = [[label_names[l] for l in label if l != 6] for label in batch[\"labels\"]]\n",
    "        true_predictions = [\n",
    "            [label_names[p] for (p, l) in zip(prediction, label) if l != 6]\n",
    "            for prediction, label in zip([predictions], batch[\"labels\"])\n",
    "        ]\n",
    "        \n",
    "        all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "        progress_bar_eval.update(1)\n",
    "        f1_now.append(all_metrics[\"overall_f1\"])\n",
    "        \n",
    "    if np.mean(f1_now) > f1_best:\n",
    "        torch.save(model_cc.state_dict(), 'sstcls_best.dat')\n",
    "        f1_best = np.mean(f1_now)\n",
    "        print(\"the best f1 is now: \"+ str(np.mean(f1_now)))\n",
    "        best_net = model_cc\n",
    "        \n",
    "\n",
    "    print(metric.compute())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d251d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80ce07ff-fed0-4a6c-9b94-25f54b05357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading the model you previously trained\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\"bert-finetuned/checkpoint-816\")\n",
    "# BATCH_SIZE = 1\n",
    "# # arguments for Trainer\n",
    "# test_args = TrainingArguments(\n",
    "#     output_dir = \"bert-finetuned-testing\",\n",
    "#     do_train = False,\n",
    "#     do_predict = True,\n",
    "#     per_device_eval_batch_size = BATCH_SIZE,   \n",
    "#     dataloader_drop_last = False    \n",
    "# )\n",
    "\n",
    "# # init trainer\n",
    "# trainer = Trainer(\n",
    "#           model = model, \n",
    "#           args = test_args, \n",
    "#           compute_metrics = compute_metrics)\n",
    "\n",
    "# test_results = trainer.predict(tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00dea989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, dataloader, gpu):\n",
    "    net.eval()\n",
    "\n",
    "    f1_now = []\n",
    "    precision_now = []\n",
    "    recall_now = []\n",
    "    accuracy_now = []\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.cuda() for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model_cc(batch['input_ids'], token_type_ids=batch['token_type_ids'], attention_mask=batch['attention_mask'], labels=batch['labels'])\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        true_labels = [[label_names[l] for l in label if l != 6] for label in batch[\"labels\"]]\n",
    "        true_predictions = [\n",
    "            [label_names[p] for (p, l) in zip(prediction, label) if l != 6]\n",
    "            for prediction, label in zip([predictions], batch[\"labels\"])\n",
    "        ]\n",
    "        \n",
    "        all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "        progress_bar_eval.update(1)\n",
    "        f1_now.append(all_metrics[\"overall_f1\"])\n",
    "        precision_now.append(all_metrics[\"overall_precision\"])\n",
    "        recall_now.append(all_metrics[\"overall_recall\"])\n",
    "        accuracy_now.append(all_metrics[\"overall_accuracy\"])\n",
    "\n",
    "\n",
    "    return np.mean(f1_now), np.mean(precision_now), np.mean(recall_now), np.mean(accuracy_now),\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aca124a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "255it [05:21, 10.47it/s]                                                       \u001b[A\n",
      "263it [05:21, 14.13it/s]\u001b[A\n",
      "271it [05:21, 18.64it/s]\u001b[A\n",
      "279it [05:21, 24.03it/s]\u001b[A\n",
      "287it [05:21, 29.84it/s]\u001b[A\n",
      "295it [05:22, 36.20it/s]\u001b[A\n",
      "303it [05:22, 42.48it/s]\u001b[A\n",
      "311it [05:22, 48.83it/s]\u001b[A\n",
      "319it [05:22, 55.11it/s]\u001b[A\n",
      "327it [05:22, 58.97it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_f1_score:::0.7975521782938886\n",
      "test_precision_score:::0.7754407642518912\n",
      "test_recall_score:::0.8283199230488388\n",
      "test_accuracy_score:::0.967577520670473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 6522/6522 [05:36<00:00, 20.77it/s]\n",
      "332it [05:36, 58.97it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "preds = predict(best_net, test_dataloader, 0)\n",
    "print(\"test_f1_score:::\"+str(preds[0]))\n",
    "print(\"test_precision_score:::\"+str(preds[1]))\n",
    "print(\"test_recall_score:::\"+str(preds[2]))\n",
    "print(\"test_accuracy_score:::\"+str(preds[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcecdc5f-ec97-4974-8170-7817127660f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
